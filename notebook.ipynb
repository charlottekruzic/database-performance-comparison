{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JEXns_C5SaXc"
   },
   "source": [
    "# Projet Traitement et Données Large Échelle\n",
    "\n",
    "Zoé MARQUIS & Charlotte KRUZIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!! Attention : Notebook très long (près d'une heure d'exécution)\n",
    "\n",
    "Cela s'explique par la nature du benchmark :\n",
    "\n",
    "- Chaque opération (Create, Read, Update, Delete) est répétée 5 fois pour assurer la fiabilité des résultats.\n",
    "- À chaque Update ou Delete (et même Select pour rester cohérent), il est nécessaire de drop la table et l’index, puis de les recréer.\n",
    "- Nous avons testé sur différents volumes de données : faible, 8k, et 30k, pour observer les variations de performances.\n",
    "- Nous avons évalué les performances avec 1, 2, et 5 réplicas.\n",
    "- Enfin, les tests ont été réalisés avec et sans index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exécution des scripts d'installation \n",
    "\n",
    "Nous avons initialement exploré la possibilité d'exécuter nos scripts sur Google Colab afin de faciliter l'exécution sans que vous ayez à configurer quoi que ce soit sur votre machine locale. Cependant, Colab présente certaines limitations, notamment l'incapacité de gérer plusieurs réplicas sets, car il ne permet de travailler qu'avec un seul environnement d'exécution.\n",
    "\n",
    "Pour pouvoir simuler plusieurs réplicas sets dans notre projet, nous avons besoin de plusieurs nœuds, ce qui n'est pas possible sur Colab. Nous avons également essayé d'utiliser des solutions gratuites comme Datastax (ou une autre plateforme similaire), mais ces services ne permettaient pas de configurer correctement les réplicas sets, ce qui limitait la flexibilité nécessaire à notre projet.\n",
    "\n",
    "En conséquence, nous avons opté pour une autre solution : créer des fichiers de configuration et utiliser un Jupyter Notebook. Cette approche permet de travailler de manière plus flexible tout en maintenant un contrôle total sur les paramètres des réplicas sets.\n",
    "\n",
    "Une autre remarque importante : le projet a été testé avec Python 3.11, car Python 3.12 a introduit quelques incompatibilités qui ont causé des bugs. Il est donc recommandé d'utiliser Python 3.11 pour éviter ces problèmes.\n",
    "\n",
    "TODO CHARLOTTE : mets le set up ubuntu en place \n",
    "\n",
    "    sudo apt ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choix de la base de données : MySQL vs SQLite\n",
    "Initialement, nous avons choisi MySQL pour ses fonctionnalités robustes et sa capacité à gérer de grandes bases de données. Cependant, son intégration à Google Colab a posé plusieurs problèmes :\n",
    "\n",
    "- Installation et configuration du serveur complexes dans un environnement éphémère.\n",
    "- Difficulté à maintenir un serveur actif sur Colab, entraînant des arrêts inattendus.\n",
    "- Configuration complexe pour établir des connexions sécurisées à distance. \n",
    "\n",
    "Pour simplifier, nous avons opté pour SQLite, une solution mieux adaptée à nos besoins :\n",
    "\n",
    "- Sans serveur : Pas de configuration complexe, tout fonctionne directement via des fichiers.\n",
    "- Compatibilité native : Intégré à Python, fonctionne localement et sur le cloud.\n",
    "\n",
    "SQLite s'est révélé être une solution efficace, ce qui nous a incités à le conserver lors de notre transition vers Jupyter Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zNbLEjRUSaXd"
   },
   "source": [
    "# Comparaison de Performances entre Systèmes Relationnels et NoSQL : Étude de Cas avec le Catalogue Netflix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wR7GJDOISaXe"
   },
   "source": [
    "Dans ce projet, nous comparons les performances entre un système de base de données relationnel (SQLite) et un système NoSQL (Cassandra), en utilisant le **catalogue Netflix** comme jeu de données.   \n",
    "Ce dataset, disponible en Open Data, offre une structure réaliste et adaptée pour explorer les différences entre ces deux approches de gestion de données.   \n",
    "L'objectif principal est d'évaluer les performances des opérations d'insertion, de sélection, de mise à jour et de suppression, tout en explorant les particularités du système NoSQL choisi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZqbpvhO7SaXe"
   },
   "source": [
    "### Jeu de données\n",
    "\n",
    "Le jeu de données utilisé contient des informations sur les films et séries disponibles sur Netflix, avec les attributs suivants : \n",
    "- `show_id` : Identifiant unique pour chaque émission ou film. \n",
    "- `type` \n",
    "- `title`\n",
    "- `director` \n",
    "- `cast`\n",
    "- `country`\n",
    "- `date_added` \n",
    "- `release_year` \n",
    "- `rating` \n",
    "- `duration` \n",
    "- `listed_in` \n",
    "- `description`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KqlM6RB7SaXe"
   },
   "source": [
    "### Schéma de données\n",
    "\n",
    "```sql\n",
    "CREATE TABLE IF NOT EXISTS shows (\n",
    "    show_id INT PRIMARY KEY,\n",
    "    title TEXT,\n",
    "    type TEXT,\n",
    "    director TEXT,\n",
    "    cast TEXT,\n",
    "    country TEXT,\n",
    "    date_added TEXT,\n",
    "    release_year INT,\n",
    "    rating TEXT,\n",
    "    duration TEXT,\n",
    "    listed_in TEXT,\n",
    "    description TEXT\n",
    ");\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xApu2UauSaXe"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Fonctionnement de Cassandra\n",
    "Apache Cassandra est un système NoSQL conçu pour gérer de grandes quantités de données de manière distribuée, garantissant haute disponibilité et tolérance aux pannes grâce à son modèle décentralisé et évolutif.\n",
    "\n",
    "### Modèle de données de Cassandra :\n",
    "\n",
    "Le modèle de données de Cassandra repose sur quelques concepts clés qui diffèrent des systèmes relationnels traditionnels.\n",
    "\n",
    "- **Keyspace** :  \n",
    "Un Keyspace dans Cassandra est l'équivalent d'une base de données dans les systèmes relationnels. Il sert à regrouper des tables (ou Column Families) et définit la stratégie de réplication des données. Par exemple, il spécifie le nombre de répliques à conserver pour chaque donnée afin d'assurer une haute disponibilité et une tolérance aux pannes. La réplication des données dans Cassandra suit des stratégies comme \"SimpleStrategy\" (réplication sur tous les nœuds) ou \"NetworkTopologyStrategy\" (réplication sur des nœuds répartis sur plusieurs centres de données). Dans ce projet, nous n'explorerons que la stratégie \"SimpleStrategy\".\n",
    "- **Column Family** :  \n",
    "Une Column Family dans Cassandra est équivalente à une table relationnelle, mais avec une flexibilité plus grande. Chaque Column Family contient plusieurs colonnes, et contrairement aux bases de données relationnelles, les colonnes n'ont pas besoin d'être définies à l'avance. Elles peuvent être ajoutées dynamiquement au fil du temps, ce qui permet d'évoluer facilement sans migrations complexes.\n",
    "- **Key (Clé primaire)** :  \n",
    "Chaque ligne dans une Column Family est identifiée par une clé unique. Cette clé est utilisée pour partitionner et localiser les données efficacement. Elle peut être composée d'un seul champ (clé primaire simple) ou de plusieurs champs (clé primaire composée).\n",
    "- **Colonnes** :  \n",
    "Les données dans Cassandra sont stockées sous forme de colonnes. Chaque colonne a trois composants essentiels :\n",
    "    - Nom : Le nom de la colonne (par exemple, title, release_year).\n",
    "    - Valeur : La donnée proprement dite (par exemple, \"Inception\", \"2010\").\n",
    "    - Timestamp : Le timestamp associé à la colonne, qui permet de gérer les versions des données et de résoudre les conflits de réplication, ce qui est essentiel dans un système distribué. Il permet de savoir quelle version d'une colonne est la plus récente.  \n",
    "Ces colonnes sont indépendantes les unes des autres. Cela signifie qu'elles peuvent être insérées, mises à jour ou supprimées sans impacter les autres colonnes dans la même ligne.\n",
    "- **Super Colonnes** :  \n",
    "Les Super Colonnes sont une structure plus avancée dans Cassandra. Elles permettent de regrouper plusieurs colonnes sous un même nom de \"super colonne\". Cela permet de créer des structures hiérarchiques ou imbriquées dans les données, où une super colonne peut contenir plusieurs colonnes. Ce mécanisme est utile pour des cas complexes, mais il est souvent moins utilisé au profit de modèles de données plus simples.\n",
    "\n",
    "Source : \n",
    "- http://www-igm.univ-mlv.fr/~dr/XPOSE2010/Cassandra/modele.html\n",
    "\n",
    "### Les différentes clefs \n",
    "\n",
    "- **Clef primaire** :  Identifie de manière unique chaque enregistrement dans la table. Elle est formée de :\n",
    "    - Clé de partition : Obligatoire.\n",
    "    - Clés de clustering : Optionnelles.\n",
    "\n",
    "- **Clef de partition** : Détermine sur quel nœud du cluster les données seront stockées. C'est la première composante de la clé primaire.\n",
    "Cassandra utilise un algorithme de hachage pour distribuer les partitions uniformément sur les nœuds.\n",
    "Tous les enregistrements ayant la même clé de partition seront stockés sur le même nœud (dans une partition).\n",
    "\n",
    "- **Clef de clustering** : Organise les données au sein d'une partition.\n",
    "Fait partie de la clé primaire, mais intervient après la clé de partition.\n",
    "Les données sont triées dans l'ordre croissant (par défaut) ou décroissant au sein de chaque partition.\n",
    "\n",
    "- **Clé secondaire** : Utilisée pour rechercher des données via une colonne qui n'est ni une clé de partition ni une clé de clustering. Cassandra génère un index secondaire pour accélérer les recherches sur cette colonne. Cependant, les performances peuvent être dégradées si l'index est utilisé pour des requêtes impliquant un balayage de nombreuses partitions. Une clé secondaire est définie en créant un **index** sur la colonne souhaitée.\n",
    "\n",
    "Source: \n",
    "- https://www.baeldung.com/cassandra-keys\n",
    "\n",
    "### Architecture de Cassandra :\n",
    "\n",
    "L'architecture de Cassandra est décentralisée et repose sur un modèle peer-to-peer, ce qui signifie qu'il n'y a pas de nœud maître ou de coordination centralisée. Tous les nœuds sont égaux et partagent le même rôle. Cela permet une grande scalabilité et une tolérance aux pannes.\n",
    "\n",
    "- **Partitions et Réplication** : Cassandra répartit les données entre différents nœuds via un mécanisme de partitionnement basé sur un hash de la clé primaire. Chaque nœud du cluster stocke une portion des données, et des répliques de ces données peuvent être présentes sur plusieurs nœuds, selon la stratégie de réplication définie dans le Keyspace. Cela permet à Cassandra d'assurer une haute disponibilité, même en cas de panne d'un ou plusieurs nœuds.\n",
    "- **Consistence et Quorum** : Cassandra suit un modèle de consistance configurable, permettant de choisir entre des garanties de consistance fortes ou une consistance plus faible en fonction des besoins. Cela se configure via les paramètres Read Consistency Level et Write Consistency Level. Par exemple, un niveau de consistance QUORUM signifie qu'une opération de lecture ou d'écriture devra être validée par la majorité des répliques d'une donnée avant de réussir.\n",
    "\n",
    "Source:\n",
    "- https://www.geeksforgeeks.org/quorum-consistency-in-cassandra/\n",
    "\n",
    "### Requêtes et Modèle de Consistance :\n",
    "\n",
    "Les requêtes dans Cassandra utilisent le Cassandra Query Language (CQL), qui ressemble à SQL mais avec des différences significatives adaptées aux particularités du modèle NoSQL.\n",
    "\n",
    "- **Clé primaire et partitions** : Dans Cassandra, la clé primaire détermine comment les données sont partitionnées et distribuées à travers les nœuds du cluster comme expliqué plus haut. Il est essentiel de bien concevoir cette clé pour assurer des performances optimales.\n",
    "- **Sélection et filtrage** : Cassandra fonctionne très bien pour les requêtes basées sur la clé primaire. Les requêtes qui filtrent sur d'autres colonnes nécessitent l'utilisation d'index secondaires ou d'une modélisation spécifique des données pour garantir de bonnes performances. Filtrer sur une colonne sans index lève l'erreur ALLOW FILTERING, ce qui signifie que la requête pourrait être très coûteuse en termes de performance.\n",
    "\n",
    "- \n",
    "\n",
    "### Avantages et Inconvénients de Cassandra :\n",
    "\n",
    "#### Avantages :\n",
    "- **Haute scalabilité horizontale** :\n",
    "    - Possibilité d'ajouter facilement des nœuds pour s'adapter à une augmentation soudaine de la demande\n",
    "    - Architecture scalable de façon linéaire permettant de déployer des clusters \"multi-node\"\n",
    "- **Résilience et tolérance aux pannes**\n",
    "    - Réplication des données sur plusieurs nœuds pour garantir une haute disponibilité\n",
    "    Absence de point unique de défaillance\n",
    "    - Capacité à continuer à fonctionner même en cas de panne d'un ou plusieurs nœuds\n",
    "- **Flexibilité du modèle de données**\n",
    "    - Prise en charge des données structurées, semi-structurées et non structurées\n",
    "    - Possibilité d'ajouter de nouvelles colonnes sans affecter les données existantes\n",
    "- **Performances élevées**\n",
    "    - Impressionnante vitesse d'écriture de données\n",
    "    - Capacité à traiter de vastes quantités de données dispersées entre de multiples serveurs\n",
    "#### Inconvénients :\n",
    "- L'un des principaux inconvénients de Cassandra réside dans la gestion des requêtes sur des colonnes non incluses dans la clé primaire, même lorsqu'un index est présent. Cela peut entraîner des problèmes de performances et nécessiter l'utilisation de la clause ALLOW FILTERING, qui est généralement déconseillée. \n",
    "Les index secondaires ne contiennent pas la clé de partition, ce qui oblige Cassandra à rechercher les données sur tous les nœuds du cluster. Cela peut être coûteux en termes de performances, surtout sur de grands clusters.\n",
    "La clause ALLOW FILTERING est nécessaire lorsque Cassandra ne peut pas garantir une exécution efficace de la requête. Cela se produit souvent pour des requêtes impliquant des comparaisons (comme >, <) ou plusieurs colonnes filtrées sans clé de partition précise. \n",
    "\n",
    "Nous avions commencé à faire les comparaison de Select, Update et Delete avec release_year > 2000, mais cela oblige l'utilisation de ALLOW FILTERING, donc ne voyions aucune différence avec et sans index. Nous avons donc tout rejoué avec = 2000 pour pouvoir voir la différence.\n",
    "\n",
    "\n",
    "Sources : \n",
    "- https://www.lebigdata.fr/apache-cassandra-definition\n",
    "- https://datascientest.com/apache-cassandra\n",
    "- https://www.scnsoft.com/data/cassandra-performance\n",
    "\n",
    "\n",
    "### Conclusion :\n",
    "Cassandra est une base de données extrêmement puissante pour les cas d'utilisation à grande échelle et hautement distribués, avec une grande tolérance aux pannes. Cependant, sa conception impose certaines contraintes sur la manière de structurer les données, et une bonne compréhension de son modèle de partitionnement est essentielle pour garantir de bonnes performances. Si l'application nécessite des requêtes complexes ou des jointures, il est déconseillé d'utiliser Cassandra, car il n'est pas conçu pour ce type de traitement sans outils supplémentaires."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "-- \n",
    "\n",
    "## Fonctionnement de SQLite\n",
    "SQLite est une base de données relationnelle légère, autonome et sans serveur, idéale pour les applications nécessitant une gestion locale des données. Voici ses principales caractéristiques :\n",
    "\n",
    "- **Base embarquée** : Contrairement à MySQL ou PostgreSQL, SQLite est intégré directement dans l'application sans serveur séparé. Les données sont stockées dans un fichier unique.\n",
    "- **Structure relationnelle** : SQLite suit le modèle relationnel classique avec tables, colonnes et lignes, et prend en charge les requêtes SQL standards (SELECT, INSERT, UPDATE, DELETE).\n",
    "- **Simplicité** : Pas de serveur à installer ni de configuration complexe, ce qui facilite son utilisation pour des prototypes, des applications mobiles ou locales.\n",
    "- **Performance** : Bien que performant pour des données locales et de petite taille, SQLite n'est pas conçu pour des applications à grande échelle nécessitant scalabilité ou haute disponibilité.\n",
    "- **Transactions ACID** : SQLite assure l'intégrité des données avec un modèle transactionnel garantissant l'atomicité et la cohérence, mais sa gestion des transactions simultanées est moins robuste que celle de systèmes plus avancés.\n",
    "\n",
    "#### Avantages\n",
    "- **Efficacité en mémoire** : SQLite nécessite peu de mémoire, ce qui la rend rapide et idéale pour des applications de toutes tailles.\n",
    "- **Autonomie** : Il fonctionne sans serveur externe, ce qui simplifie son intégration et son utilisation.\n",
    "- **Polyvalence** : SQLite supporte les commandes SQL standard et est compatible avec de nombreux formats de données, ce qui le rend populaire dans diverses applications comme Facebook ou WhatsApp.\n",
    "- **Portabilité** : Les fichiers SQLite sont faciles à sauvegarder et transférer, sans dépendance à un serveur spécifique.\n",
    "- **Fiabilité** : Moins sujet aux erreurs de mémoire ou limitations liées à la RAM, avec une faible consommation de ressources.\n",
    "- **Libre de droits** : SQLite est en domaine public, sans licence, ce qui en fait une solution économique.\n",
    "\n",
    "#### Limites\n",
    "- **Absence de gestion multi-utilisateurs** : SQLite ne supporte pas plusieurs utilisateurs ou connexions simultanées, ce qui le rend limité pour les applications multi-clients.\n",
    "- **Impact de la croissance des données** : La performance peut diminuer avec des volumes importants de données.\n",
    "- **Limitation des requêtes client** : SQLite ne gère pas bien les requêtes client directes ou les connexions simultanées, ce qui peut causer des retards dans certaines applications.\n",
    "\n",
    "\n",
    "Sources : \n",
    "- https://www.ionos.fr/digitalguide/sites-internet/developpement-web/sqlite/\n",
    "- https://datascientest.com/sqlite-tout-savoir\n",
    "- https://www.tutlane.com/tutorial/sqlite/sqlite-acid-transactions\n",
    "- https://blog.stephane-robert.info/docs/services/bdd/relationnelles/sqlite/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CSf1vpGsSce3"
   },
   "source": [
    "---\n",
    "## Installer les outils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "186hCryRIqD6",
    "outputId": "3c35e481-857d-48f9-8a48-49f0cb5b74d4"
   },
   "outputs": [],
   "source": [
    "!pip install plotly matplotlib pandas numpy cassandra_driver kagglehub tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importer toutes les librairies nécessaires\n",
    "from cassandra.cluster import Cluster\n",
    "\n",
    "import kagglehub\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "from tabulate import tabulate\n",
    "import time\n",
    "import tracemalloc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSa5TnVxSaXf"
   },
   "source": [
    "## Importation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7EWeXgMVTYUR",
    "outputId": "92b8b7cb-3191-4cdc-dfe8-0435ea2c0220"
   },
   "outputs": [],
   "source": [
    "path = kagglehub.dataset_download(\"shivamb/netflix-shows\")\n",
    "print(\"Chemin vers le fichier du dataset : \", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "31NMsovvTYM6",
    "outputId": "d9527294-d00e-490b-b3f3-936eba3790fd"
   },
   "outputs": [],
   "source": [
    "files = os.listdir(path)\n",
    "print(\"Nom du fichier : \", files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Bvv23wiLSaXg"
   },
   "outputs": [],
   "source": [
    "filename = f\"{path}/{files[0]}\"\n",
    "df_initial = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_npbZAVzSaXg",
    "outputId": "a455d3b7-8c08-4319-f298-2f0e93c112af"
   },
   "outputs": [],
   "source": [
    "print(tabulate(df_initial.head(10), headers='keys', tablefmt='psql'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LhIN7tF2UP3e",
    "outputId": "dd3f1bd5-2061-4a92-9b5c-d9a706660362"
   },
   "outputs": [],
   "source": [
    "# afficher le nombre de lignes dans le dataset\n",
    "print(f\"Nombre de lignes : {len(df_initial)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ja5uNKumUP1X",
    "outputId": "85d58bb2-0c66-4f47-c720-ce978e0a9f2e"
   },
   "outputs": [],
   "source": [
    "# types des colonnes\n",
    "print(df_initial.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "QEm7E9xvSaXg"
   },
   "outputs": [],
   "source": [
    "# Convertir la colonne show_id en int en enlevant le préfixe 's' \n",
    "# (plus simple pour gérer exactement le meme type de données en Cassandra et SQLite)\n",
    "df_initial['show_id'] = df_initial['show_id'].str.replace('s', '').astype(int).astype(int)\n",
    "\n",
    "df_initial['show_id'] = df_initial['show_id'].astype(int)\n",
    "df_initial['release_year'] = df_initial['release_year'].astype(int)\n",
    "\n",
    "df_initial['type'] = df_initial['title'].astype(str)\n",
    "df_initial['title'] = df_initial['title'].astype(str)\n",
    "df_initial['director'] = df_initial['director'].astype(str)\n",
    "df_initial['cast'] = df_initial['cast'].astype(str)\n",
    "df_initial['country'] = df_initial['country'].astype(str)\n",
    "df_initial['date_added'] = df_initial['date_added'].astype(str)\n",
    "df_initial['rating'] = df_initial['rating'].astype(str)\n",
    "df_initial['duration'] = df_initial['duration'].astype(str)\n",
    "df_initial['listed_in'] = df_initial['listed_in'].astype(str)\n",
    "df_initial['description'] = df_initial['description'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nhGD7JxiUPy_",
    "outputId": "14418a1c-e7a1-4425-ebb9-b36e5164063e"
   },
   "outputs": [],
   "source": [
    "# types des colonnes\n",
    "print(df_initial.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uDH9Xdn3aRcT"
   },
   "source": [
    "## CRUD : Create, Read, Update, Delete\n",
    "- Create (*Insert*) : Insérer des données dans la base de données.\n",
    "- Read (*Select*) : Récupérer des données.\n",
    "- Update (*Update*) : Modifier des données existantes.\n",
    "- Delete (*Delete*) : Effacer des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "v-XAjHUPL-Fq"
   },
   "outputs": [],
   "source": [
    "schema = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS shows (\n",
    "    show_id INT PRIMARY KEY,\n",
    "    title TEXT,\n",
    "    type TEXT,\n",
    "    director TEXT,\n",
    "    cast TEXT,\n",
    "    country TEXT,\n",
    "    date_added TEXT,\n",
    "    release_year INT,\n",
    "    rating TEXT,\n",
    "    duration TEXT,\n",
    "    listed_in TEXT,\n",
    "    description TEXT\n",
    ");\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "5HsvkRtyOTPO"
   },
   "outputs": [],
   "source": [
    "# création d'une donnée de test : une ligne pour le film Zoé & Charlotte\n",
    "data_zoe_charlotte = {\n",
    "    'show_id': [0],\n",
    "    'title': ['Zoé & Charlotte'],\n",
    "    'type': ['Movie'],\n",
    "    'director': ['Christopher Nolan'],\n",
    "    'cast': ['Leonardo DiCaprio, Joseph Gordon-Levitt'],\n",
    "    'country': ['USA'],\n",
    "    'date_added': ['2021-01-01'],\n",
    "    'release_year': [2000],\n",
    "    'rating': ['PG-13'],\n",
    "    'duration': ['148 min'],\n",
    "    'listed_in': ['Action, Sci-Fi'],\n",
    "    'description': ['A thief who steals corporate secrets through the use of dream-sharing technology is given the inverse task of planting an idea into the mind of a CEO.']\n",
    "}\n",
    "df_zoe_charlotte = pd.DataFrame(data_zoe_charlotte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gyQhyPcqXcdW"
   },
   "source": [
    "### SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "czfGeguOZ7zT",
    "outputId": "937e55dc-b9ad-449c-93ad-2fe850ba5215"
   },
   "outputs": [],
   "source": [
    "# Créer une base de données SQLite en mémoire\n",
    "conn = sqlite3.connect(':memory:')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "def drop_table_sqlite():\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS shows;\")\n",
    "    conn.commit()\n",
    "\n",
    "def create_table_sqlite():\n",
    "    # Crée ta table ici\n",
    "    cursor.execute(schema)\n",
    "    conn.commit()\n",
    "\n",
    "drop_table_sqlite()\n",
    "create_table_sqlite()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "v4PSpmxUNXwJ"
   },
   "outputs": [],
   "source": [
    "# SQLite : fonctions pour insérer, lire, mettre à jour et supprimer des données à utiliser pour le benchmark\n",
    "# on récupère le temps, la mémoire courante et la mémoire maximale utilisée pour chaque opération\n",
    "\n",
    "# --- CREATE ---\n",
    "def sqlite_insert(df):\n",
    "    drop_table_sqlite()\n",
    "    create_table_sqlite()\n",
    "    columns = ', '.join(df.columns)\n",
    "    placeholders = ', '.join(['?'] * len(df.columns))\n",
    "    insert_query = f\"INSERT INTO shows ({columns}) VALUES ({placeholders})\"\n",
    "\n",
    "    tracemalloc.start()\n",
    "    start_time = time.time()\n",
    "    for _, row in df.iterrows():\n",
    "      cursor.execute(insert_query, tuple(row))\n",
    "    conn.commit()\n",
    "\n",
    "    create_time_sqlite = time.time() - start_time\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    current = current / 1024 / 1024\n",
    "    peak = peak / 1024 / 1024\n",
    "    tracemalloc.stop()\n",
    "    print(f\"Create Time: {create_time_sqlite:.6f} seconds\")\n",
    "    print(f\"Current memory usage is {current}Mo; Peak was {peak}Mo\")\n",
    "    return create_time_sqlite, current, peak\n",
    "\n",
    "# --- READ ---\n",
    "def sqlite_read(df):\n",
    "    sqlite_insert(df)\n",
    "\n",
    "    tracemalloc.start()\n",
    "    start_time = time.time()\n",
    "    cursor.execute(\"SELECT * FROM shows WHERE release_year = 2000\")\n",
    "    result = cursor.fetchall()\n",
    "    read_time_sqlite = time.time() - start_time\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    current = current / 1024 / 1024\n",
    "    peak = peak / 1024 / 1024\n",
    "    tracemalloc.stop()\n",
    "    print(f\"Read Time: {read_time_sqlite:.6f} seconds\")\n",
    "    print(f\"Queried {len(result)} records\")\n",
    "    print(f\"Current memory usage is {current}Mo; Peak was {peak}Mo\")\n",
    "    return read_time_sqlite, current, peak\n",
    "\n",
    "# --- UPDATE ---\n",
    "def sqlite_update(df):\n",
    "    sqlite_insert(df)\n",
    "\n",
    "    tracemalloc.start() \n",
    "    start_time = time.time()\n",
    "    cursor.execute(\"UPDATE shows SET rating='PG' WHERE release_year = 2000\")\n",
    "    conn.commit()\n",
    "    update_time_sqlite = time.time() - start_time\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    current = current / 1024 / 1024\n",
    "    peak = peak / 1024 / 1024\n",
    "    tracemalloc.stop()\n",
    "\n",
    "    print(f\"Update Time: {update_time_sqlite:.6f} seconds\")\n",
    "    print(f\"Current memory usage is {current}Mo; Peak was {peak}Mo\")\n",
    "    return update_time_sqlite, current, peak\n",
    "\n",
    "# --- DELETE ---\n",
    "def sqlite_delete(df):\n",
    "    sqlite_insert(df)\n",
    "\n",
    "    tracemalloc.start()\n",
    "    start_time = time.time()\n",
    "    cursor.execute(\"DELETE FROM shows WHERE release_year = 2000\")\n",
    "    conn.commit()\n",
    "    delete_time_sqlite = time.time() - start_time\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    current = current / 1024 / 1024\n",
    "    peak = peak / 1024 / 1024\n",
    "    tracemalloc.stop()\n",
    "    print(f\"Delete Time: {delete_time_sqlite:.6f} seconds\")\n",
    "    print(f\"Current memory usage is {current}Mo; Peak was {peak}Mo\")\n",
    "    return delete_time_sqlite, current, peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour effectuer une opération plusieurs fois et calculer la moyenne et l'écart-type\n",
    "def benchmark_operation(func, *args):\n",
    "    times = []\n",
    "    current_memory = []\n",
    "    peak_memory = []\n",
    "    for _ in range(5):  # Effectuer 5 itérations\n",
    "        temps, current, peak = func(*args)  # Exécuter la fonction avec les arguments\n",
    "        times.append(temps)\n",
    "        current_memory.append(current)\n",
    "        peak_memory.append(peak)\n",
    "    \n",
    "    # Calculer la moyenne et l'écart-type\n",
    "    mean_time = np.mean(times)\n",
    "    std_time = np.std(times)\n",
    "    mean_current_memory = np.mean(current_memory)\n",
    "    std_current_memory = np.std(current_memory)\n",
    "    mean_peak_memory = np.mean(peak_memory)\n",
    "    std_peak_memory = np.std(peak_memory)\n",
    "    return mean_time, std_time, mean_current_memory, std_current_memory, mean_peak_memory, std_peak_memory\n",
    "\n",
    "# --- CREATE ---\n",
    "create_sqlite_mean_time, create_sqlite_std_time, \\\n",
    "    create_sqlite_mean_current_memory, create_sqlite_std_current_memory, \\\n",
    "        create_sqlite_mean_peak_memory, create_sqlite_std_peak_memory = benchmark_operation(sqlite_insert, df_zoe_charlotte)\n",
    "\n",
    "# --- READ ---\n",
    "read_sqlite_mean_time, read_sqlite_std_time, \\\n",
    "    read_sqlite_mean_current_memory, read_sqlite_std_current_memory, \\\n",
    "        read_sqlite_mean_peak_memory, read_sqlite_std_peak_memory = benchmark_operation(sqlite_read, df_zoe_charlotte)\n",
    "\n",
    "# --- UPDATE ---\n",
    "update_sqlite_mean_time, update_sqlite_std_time, \\\n",
    "    update_sqlite_mean_current_memory, update_sqlite_std_current_memory, \\\n",
    "        update_sqlite_mean_peak_memory, update_sqlite_std_peak_memory = benchmark_operation(sqlite_update, df_zoe_charlotte)\n",
    "\n",
    "# --- DELETE ---\n",
    "delete_sqlite_mean_time, delete_sqlite_std_time, \\\n",
    "    delete_sqlite_mean_current_memory, delete_sqlite_std_current_memory, \\\n",
    "        delete_sqlite_mean_peak_memory, delete_sqlite_std_peak_memory = benchmark_operation(sqlite_delete, df_zoe_charlotte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8p6QWjBaUO8t"
   },
   "source": [
    "##### Test d'insertion avec une ligne de DataFrame\n",
    "Nous avons d'abord testé l'insertion d'une seule ligne de données extraite d'un DataFrame dans notre base de données. \n",
    "\n",
    "Cette étape permet de vérifier que l'insertion fonctionne correctement sur un cas simple, avant de passer à l'insertion de l'ensemble du DataFrame.\n",
    "\n",
    "À ce stade, l'insertion a été réalisée avec succès sur une ligne de données, ce qui nous permet d'assurer que la logique de base est correcte. \n",
    "\n",
    "Nous allons maintenant procéder à l'insertion du DataFrame complet pour tester la gestion de plusieurs lignes et évaluer les performances d'insertion à plus grande échelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CREATE ---\n",
    "create_sqlite_massive_mean_time, create_sqlite_massive_std_time, \\\n",
    "    create_sqlite_massive_mean_current_memory, create_sqlite_massive_std_current_memory,\\\n",
    "    create_sqlite_massive_mean_peak_memory, create_sqlite_massive_std_peak_memory  = benchmark_operation(sqlite_insert, df_initial)\n",
    "\n",
    "# --- READ ---\n",
    "read_sqlite_massive_mean_time, read_sqlite_massive_std_time, \\\n",
    "    read_sqlite_massive_mean_current_memory, read_sqlite_massive_std_current_memory,\\\n",
    "    read_sqlite_massive_mean_peak_memory, read_sqlite_massive_std_peak_memory = benchmark_operation(sqlite_read, df_initial)\n",
    "\n",
    "# --- UPDATE ---\n",
    "update_sqlite_massive_mean_time, update_sqlite_massive_std_time, \\\n",
    "    update_sqlite_massive_mean_current_memory, update_sqlite_massive_std_current_memory,\\\n",
    "    update_sqlite_massive_mean_peak_memory, update_sqlite_massive_std_peak_memory = benchmark_operation(sqlite_update, df_initial)\n",
    "\n",
    "# --- DELETE ---\n",
    "delete_sqlite_massive_mean_time, delete_sqlite_massive_std_time, \\\n",
    "    delete_sqlite_massive_mean_current_memory, delete_sqlite_massive_std_current_memory,\\\n",
    "    delete_sqlite_massive_mean_peak_memory, delete_sqlite_massive_std_peak_memory = benchmark_operation(sqlite_delete, df_initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lvIFGYwhtkfY"
   },
   "source": [
    "### Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vérifier l'état actuel du noeud Cassandra\n",
    "!nodetool status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 608
    },
    "id": "sZNeaq6fwhS9",
    "outputId": "6b6d4186-8a12-4942-aea8-6806c6ee8835"
   },
   "outputs": [],
   "source": [
    "# Connexion à Cassandra\n",
    "cluster = Cluster(['127.0.0.1'])\n",
    "session = cluster.connect()\n",
    "\n",
    "\n",
    "def drop_table_cassandra():\n",
    "    session.execute(\"\"\"\n",
    "        DROP TABLE IF EXISTS netflix.shows;\n",
    "        \"\"\")\n",
    "    session.execute(\"\"\"\n",
    "        DROP KEYSPACE IF EXISTS netflix;\n",
    "        \"\"\")\n",
    "\n",
    "def create_table_cassandra(replication_factor):\n",
    "    session.execute(f\"\"\"\n",
    "        CREATE KEYSPACE netflix\n",
    "        WITH REPLICATION = {{\n",
    "            'class': 'SimpleStrategy', \n",
    "            'replication_factor': {replication_factor}\n",
    "        }};\n",
    "        \"\"\")\n",
    "    session.set_keyspace('netflix')\n",
    "    session.execute(schema)\n",
    "    print(f\"replication factor = {replication_factor}\")\n",
    "\n",
    "     \n",
    "drop_table_cassandra()\n",
    "create_table_cassandra(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Df27nDd5-sx"
   },
   "source": [
    "##### Test sans index\n",
    "Pour l'instant, nous effectuons les tests sans index. Cette approche nous permettra d'évaluer les performances de base. Nous ajouterons ensuite des index pour comparer les résultats et analyser l'impact sur les performances des requêtes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "o5dNeAZ9tndK"
   },
   "outputs": [],
   "source": [
    "# Cassandra : fonctions pour insérer, lire, mettre à jour et supprimer des données à utiliser pour le benchmark\n",
    "# on récupère le temps, la mémoire courante et la mémoire maximale utilisée pour chaque opération\n",
    "\n",
    "# nb_repl : pour la suite, on va tester avec 1, 2 et 5 replica\n",
    "\n",
    "# Attention ici aux commentaires : on a besoin d'utiliser allow filtering, et de faire un select avant de faire un update ou un delete : \n",
    "# on explique pourquoi plus loin \n",
    "\n",
    "# --- CREATE ---\n",
    "def cassandra_insert(*args):\n",
    "    df = args[0]  # Premier argument, qui est le DataFrame\n",
    "    nb_repl = args[1]  # Deuxième argument, qui est le nombre de répliques\n",
    "\n",
    "    drop_table_cassandra()\n",
    "    create_table_cassandra(nb_repl)\n",
    "\n",
    "    insert_query = session.prepare(\"INSERT INTO shows (show_id, title, director, cast, country, date_added, release_year, rating, duration, listed_in, description) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\")\n",
    "\n",
    "    tracemalloc.start()\n",
    "    start_time = time.time()\n",
    "    for _, row in df.iterrows():\n",
    "        session.execute(insert_query, (row['show_id'], row['title'], row['director'], row['cast'], row['country'], row['date_added'], row['release_year'], row['rating'], row['duration'], row['listed_in'], row['description']))\n",
    "    create_time = time.time() - start_time\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    current = current / 1024 / 1024\n",
    "    peak = peak / 1024 / 1024\n",
    "    tracemalloc.stop()\n",
    "\n",
    "    print(f\"Create Time: {create_time:.6f} seconds\")\n",
    "    print(f\"Current memory usage is {current}Mo; Peak was {peak}Mo\")\n",
    "\n",
    "    return create_time, current, peak\n",
    "\n",
    "# --- READ ---\n",
    "def cassandra_read(*args):\n",
    "    df = args[0]  # Premier argument, qui est le DataFrame\n",
    "    nb_repl = args[1]  # Deuxième argument, qui est le nombre de répliques\n",
    "\n",
    "    cassandra_insert(df, nb_repl)\n",
    "\n",
    "    tracemalloc.start()\n",
    "    start_time = time.time()\n",
    "    # On utilise ici ALLOW FILTERING car release_year ne fait pas partie de la clef primaire et n'est pas indexé\n",
    "    result = session.execute(\"SELECT * FROM shows WHERE release_year = 2000 ALLOW FILTERING\")\n",
    "    read_time = time.time() - start_time\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    current = current / 1024 / 1024\n",
    "    peak = peak / 1024 / 1024\n",
    "    tracemalloc.stop()\n",
    "    print(f\"Read Time: {read_time:.6f} seconds\")\n",
    "    num_records = sum(1 for _ in result)  # Count rows using a generator expression\n",
    "    print(f\"Queried {num_records} records\")\n",
    "    print(f\"Current memory usage is {current}Mo; Peak was {peak}Mo\")\n",
    "    return read_time, current, peak\n",
    "\n",
    "# --- UPDATE ---\n",
    "def cassandra_update(*args):\n",
    "    df = args[0]  # Premier argument, qui est le DataFrame\n",
    "    nb_repl = args[1]  # Deuxième argument, qui est le nombre de répliques\n",
    "\n",
    "    cassandra_insert(df, nb_repl)\n",
    "\n",
    "    tracemalloc.start()\n",
    "    # On doit faire select et update car realease_year ne fait pas partie de la clef primaire\n",
    "    start_time = time.time()\n",
    "\n",
    "    select_query = \"SELECT show_id FROM shows WHERE release_year = 2000 ALLOW FILTERING;\" \n",
    "    result = session.execute(select_query)\n",
    "    show_ids_to_update = [row.show_id for row in result]\n",
    "\n",
    "    for show_id in show_ids_to_update:\n",
    "        update_query = f\"UPDATE shows SET rating = 'PG' WHERE show_id = {show_id};\"\n",
    "        session.execute(update_query)\n",
    "\n",
    "    update_time = time.time() - start_time\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    current = current / 1024 / 1024\n",
    "    peak = peak / 1024 / 1024\n",
    "    tracemalloc.stop()\n",
    "    print(f\"Update Time: {update_time:.6f} seconds\")\n",
    "    print(f\"Current memory usage is {current}Mo; Peak was {peak}Mo\")\n",
    "    return update_time, current, peak\n",
    "\n",
    "# --- DELETE ---\n",
    "def cassandra_delete(*args):\n",
    "    df = args[0]  # Premier argument, qui est le DataFrame\n",
    "    nb_repl = args[1]  # Deuxième argument, qui est le nombre de répliques\n",
    "\n",
    "    cassandra_insert(df, nb_repl)\n",
    "    \n",
    "    # Comme pour update\n",
    "    tracemalloc.start()\n",
    "    start_time = time.time()\n",
    "\n",
    "    select_query = \"SELECT show_id FROM shows WHERE release_year = 2000 ALLOW FILTERING;\"\n",
    "    result = session.execute(select_query)\n",
    "    show_ids_to_delete = [row.show_id for row in result]\n",
    "\n",
    "    for show_id in show_ids_to_delete:\n",
    "        delete_query = f\"DELETE FROM shows WHERE show_id = {show_id};\"\n",
    "        session.execute(delete_query)\n",
    "\n",
    "    delete_time = time.time() - start_time\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    current = current / 1024 / 1024\n",
    "    peak = peak / 1024 / 1024\n",
    "    tracemalloc.stop()\n",
    "    print(f\"Delete Time: {delete_time:.6f} seconds\")\n",
    "    print(f\"Current memory usage is {current}Mo; Peak was {peak}Mo\")\n",
    "    return delete_time, current, peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CREATE ---\n",
    "create_cas_mean_time, create_cas_std_time, \\\n",
    "    create_cas_mean_current_memory, create_cas_std_current_memory, \\\n",
    "        create_cas_mean_peak_memory, create_cas_std_peak_memory  = benchmark_operation(cassandra_insert, df_zoe_charlotte, 1)\n",
    "\n",
    "# --- READ ---\n",
    "read_cas_mean_time, read_cas_std_time, read_cas_mean_current_memory, \\\n",
    "    read_cas_std_current_memory, read_cas_mean_peak_memory, \\\n",
    "        read_cas_std_peak_memory  = benchmark_operation(cassandra_read, df_zoe_charlotte, 1)\n",
    "\n",
    "# --- UPDATE ---\n",
    "update_cas_mean_time, update_cas_std_time, \\\n",
    "    update_cas_mean_current_memory, update_cas_std_current_memory, \\\n",
    "        update_cas_mean_peak_memory, update_cas_std_peak_memory  = benchmark_operation(cassandra_update, df_zoe_charlotte, 1)\n",
    "\n",
    "# --- DELETE ---\n",
    "delete_cas_mean_time, delete_cas_std_time,  \\\n",
    "    delete_cas_mean_current_memory, delete_cas_std_current_memory,  \\\n",
    "        delete_cas_mean_peak_memory, delete_cas_std_peak_memory  = benchmark_operation(cassandra_delete, df_zoe_charlotte, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CREATE ---\n",
    "create_cas_massive_mean_time, create_cas_massive_std_time, \\\n",
    "    create_cas_massive_mean_current_memory, create_cas_massive_std_current_memory, \\\n",
    "        create_cas_massive_mean_peak_memory, create_cas_massive_std_peak_memory = benchmark_operation(cassandra_insert, df_initial, 1)\n",
    "\n",
    "# --- READ ---\n",
    "read_cas_massive_mean_time, read_cas_massive_std_time, \\\n",
    "    read_cas_massive_mean_current_memory, read_cas_massive_std_current_memory, \\\n",
    "        read_cas_massive_mean_peak_memory, read_cas_massive_std_peak_memory = benchmark_operation(cassandra_read, df_initial, 1)\n",
    "\n",
    "# --- UPDATE ---\n",
    "update_cas_massive_mean_time, update_cas_massive_std_time, \\\n",
    "    update_cas_massive_mean_current_memory, update_cas_massive_std_current_memory, \\\n",
    "        update_cas_massive_mean_peak_memory, update_cas_massive_std_peak_memory = benchmark_operation(cassandra_update, df_initial, 1)\n",
    "\n",
    "# --- DELETE ---\n",
    "delete_cas_massive_mean_time, delete_cas_massive_std_time, \\\n",
    "    delete_cas_massive_mean_current_memory, delete_cas_massive_std_current_memory, \\\n",
    "        delete_cas_massive_mean_peak_memory, delete_cas_massive_std_peak_memory = benchmark_operation(cassandra_delete, df_initial, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "41x937O1jknt"
   },
   "source": [
    "### Visualisation\n",
    "Nous avons d'abord testé avec un seul replicaSet et sans index, en insérant une seule ligne de données pour simplifier les tests.\n",
    "\n",
    "Ensuite, pour obtenir des statistiques plus réalistes, nous avons effectué des opérations CRUD sur un volume de données plus important, en utilisant les données de Kaggle précédemment chargées. \n",
    "\n",
    "Chaque action (insertion, lecture, mise à jour, suppression) a été réalisée cinq fois, et nous avons calculé la moyenne et l'écart-type pour chaque opération afin d'effectuer un benchmarking précis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valeurs de temps pour SQLite\n",
    "sqlite_times = {\n",
    "    'CREATE': (create_sqlite_mean_time, create_sqlite_std_time),\n",
    "    'READ': (read_sqlite_mean_time, read_sqlite_std_time),\n",
    "    'UPDATE': (update_sqlite_mean_time, update_sqlite_std_time),\n",
    "    'DELETE': (delete_sqlite_mean_time, delete_sqlite_std_time)\n",
    "}\n",
    "\n",
    "# Valeurs de temps pour Cassandra avec 1 réplicat\n",
    "cassandra_times = {\n",
    "    'CREATE': (create_cas_mean_time, create_cas_std_time),\n",
    "    'READ': (read_cas_mean_time, read_cas_std_time),\n",
    "    'UPDATE': (update_cas_mean_time, update_cas_std_time),\n",
    "    'DELETE': (delete_cas_mean_time, delete_cas_std_time)\n",
    "}\n",
    "\n",
    "operations = ['CREATE', 'READ', 'UPDATE', 'DELETE']\n",
    "\n",
    "sqlite_means = [sqlite_times[op][0] for op in operations]\n",
    "sqlite_stds = [sqlite_times[op][1] for op in operations]\n",
    "\n",
    "cassandra_means = [cassandra_times[op][0] for op in operations]\n",
    "cassandra_stds = [cassandra_times[op][1] for op in operations]\n",
    "\n",
    "y = np.arange(len(operations))\n",
    "height = 0.35\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "bars1 = ax.barh(y - height / 2, sqlite_means, height, xerr=sqlite_stds, label='SQLite', capsize=5, color='tab:blue')\n",
    "bars2 = ax.barh(y + height / 2, cassandra_means, height, xerr=cassandra_stds, label='Cassandra (1 réplicat)', capsize=5, color='tab:red')\n",
    "\n",
    "ax.set_xlabel('Temps (secondes)')\n",
    "ax.set_ylabel('Opérations')\n",
    "ax.set_title('Comparaison des performances de SQLite et Cassandra avec 1 réplicat') \n",
    "ax.set_yticks(y)\n",
    "ax.set_yticklabels(operations)\n",
    "ax.legend()\n",
    "\n",
    "def add_values(bars):\n",
    "    for bar in bars:\n",
    "        xval = bar.get_width()\n",
    "        ax.text(xval, bar.get_y() + bar.get_height() / 2, f'{xval:.6f}', va='center', ha='left')\n",
    "\n",
    "add_values(bars1)\n",
    "add_values(bars2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlite_memory = {\n",
    "    'CREATE': {'current': (create_sqlite_mean_current_memory, create_sqlite_std_current_memory),\n",
    "               'peak': (create_sqlite_mean_peak_memory, create_sqlite_std_peak_memory)},\n",
    "    'READ': {'current': (read_sqlite_mean_current_memory, read_sqlite_std_current_memory),\n",
    "             'peak': (read_sqlite_mean_peak_memory, read_sqlite_std_peak_memory)},\n",
    "    'UPDATE': {'current': (update_sqlite_mean_current_memory, update_sqlite_std_current_memory),\n",
    "               'peak': (update_sqlite_mean_peak_memory, update_sqlite_std_peak_memory)},\n",
    "    'DELETE': {'current': (delete_sqlite_mean_current_memory, delete_sqlite_std_current_memory),\n",
    "               'peak': (delete_sqlite_mean_peak_memory, delete_sqlite_std_peak_memory)}\n",
    "}\n",
    "\n",
    "cassandra_memory = {\n",
    "    'CREATE': {'current': (create_cas_mean_current_memory, create_cas_std_current_memory),\n",
    "               'peak': (create_cas_mean_peak_memory, create_cas_std_peak_memory)},\n",
    "    'READ': {'current': (read_cas_mean_current_memory, read_cas_std_current_memory),\n",
    "             'peak': (read_cas_mean_peak_memory, read_cas_std_peak_memory)},\n",
    "    'UPDATE': {'current': (update_cas_mean_current_memory, update_cas_std_current_memory),\n",
    "               'peak': (update_cas_mean_peak_memory, update_cas_std_peak_memory)},\n",
    "    'DELETE': {'current': (delete_cas_mean_current_memory, delete_cas_std_current_memory),\n",
    "               'peak': (delete_cas_mean_peak_memory, delete_cas_std_peak_memory)}\n",
    "}\n",
    "\n",
    "operations = ['CREATE', 'READ', 'UPDATE', 'DELETE']\n",
    "\n",
    "sqlite_current_means = [sqlite_memory[op]['current'][0] for op in operations]\n",
    "sqlite_current_stds = [sqlite_memory[op]['current'][1] for op in operations]\n",
    "sqlite_peak_means = [sqlite_memory[op]['peak'][0] for op in operations]\n",
    "sqlite_peak_stds = [sqlite_memory[op]['peak'][1] for op in operations]\n",
    "\n",
    "cassandra_current_means = [cassandra_memory[op]['current'][0] for op in operations]\n",
    "cassandra_current_stds = [cassandra_memory[op]['current'][1] for op in operations]\n",
    "cassandra_peak_means = [cassandra_memory[op]['peak'][0] for op in operations]\n",
    "cassandra_peak_stds = [cassandra_memory[op]['peak'][1] for op in operations]\n",
    "\n",
    "y = np.arange(len(operations))  \n",
    "height = 0.2  \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "bars_sqlite_current = ax.barh(y + 1.5 * height, sqlite_current_means, height, \n",
    "                              xerr=sqlite_current_stds, label='SQLite Current Memory', color='tab:blue', capsize=5)\n",
    "bars_cassandra_current = ax.barh(y + 0.5 * height, cassandra_current_means, height, \n",
    "                                  xerr=cassandra_current_stds, label='Cassandra Current Memory', color='red', capsize=5)\n",
    "bars_sqlite_peak = ax.barh(y - 0.5 * height, sqlite_peak_means, height, \n",
    "                           xerr=sqlite_peak_stds, label='SQLite Peak Memory', color='#89CFF0', capsize=5) # bleu clair\n",
    "bars_cassandra_peak = ax.barh(y - 1.5 * height, cassandra_peak_means, height, \n",
    "                               xerr=cassandra_peak_stds, label='Cassandra Peak Memory', color='#FFCCCB', capsize=5) # rouge clair\n",
    "\n",
    "ax.set_xlabel('Mémoire utilisée (Mo)')\n",
    "ax.set_ylabel('Opérations CRUD')\n",
    "ax.set_title('Comparaison des mémoires utilisées : SQLite vs Cassandra')\n",
    "ax.set_yticks(y)\n",
    "ax.set_yticklabels(operations)\n",
    "ax.legend()\n",
    "\n",
    "def add_values(bars):\n",
    "    for bar in bars:\n",
    "        xval = bar.get_width()\n",
    "        ax.text(xval, bar.get_y() + bar.get_height() / 2, f'{xval:.2f}', va='center', ha='left')\n",
    "\n",
    "add_values(bars_sqlite_current)\n",
    "add_values(bars_cassandra_current)\n",
    "add_values(bars_sqlite_peak)\n",
    "add_values(bars_cassandra_peak)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En testant SQLite et Cassandra avec une seule ligne de données et une réplication pour Cassandra, on remarque des différences claires :\n",
    " \n",
    "- SQLite :  \n",
    "Ici, l'opération la plus lente est la création de la ligne de données. En revanche, les lectures, mises à jour et suppressions sont rapides, ce qui montre que SQLite est bien optimisé pour des petits volumes de données.  \n",
    "- Cassandra :  \n",
    "La lecture est l'opération la plus rapide, ce qui correspond bien à la manière dont Cassandra est conçu pour gérer les requêtes en lecture.  \n",
    "Les mises à jour et suppressions prennent à peu près le même temps, et sont un peu plus rapides que la création.  \n",
    "La création, par contre, est la plus lente, probablement à cause de la gestion de la réplication et des contraintes liées à son architecture distribuée.    \n",
    "\n",
    "On fait le même constat concernant la mémoire (current et peak), où Cassandra est beaucoup plus gourmand que SQLite, notamment avec un pic énorme en lecture pour Cassandra.\n",
    "\n",
    "Globalement, avec une seule ligne de données, Cassandra est beaucoup plus lent que SQLite. Cela s’explique par son architecture distribuée, qui ajoute un overhead même dans des contextes simples, et par ses mécanismes de réplication et de gestion des partitions, conçus pour des scénarios à grande échelle. En revanche, SQLite, étant une solution locale et légère, est optimisé pour des opérations rapides sur de petites bases de données.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Données des temps de SQLite et Cassandra pour chaque opération\n",
    "sqlite_times = {\n",
    "    'CREATE': (create_sqlite_massive_mean_time, create_sqlite_massive_std_time),\n",
    "    'READ': (read_sqlite_massive_mean_time, read_sqlite_massive_std_time),\n",
    "    'UPDATE': (update_sqlite_massive_mean_time, update_sqlite_massive_std_time),\n",
    "    'DELETE': (delete_sqlite_massive_mean_time, delete_sqlite_massive_std_time)\n",
    "}\n",
    "\n",
    "cassandra_times = {\n",
    "    'CREATE': (create_cas_massive_mean_time, create_cas_massive_std_time),\n",
    "    'READ': (read_cas_massive_mean_time, read_cas_massive_std_time),\n",
    "    'UPDATE': (update_cas_massive_mean_time, update_cas_massive_std_time),\n",
    "    'DELETE': (delete_cas_massive_mean_time, delete_cas_massive_std_time)\n",
    "}\n",
    "\n",
    "operations = ['CREATE', 'READ', 'UPDATE', 'DELETE']\n",
    "\n",
    "# Extraire les moyennes et les écarts-types pour chaque opération\n",
    "sqlite_means = [sqlite_times[op][0] for op in operations]\n",
    "sqlite_stds = [sqlite_times[op][1] for op in operations]\n",
    "\n",
    "cassandra_means = [cassandra_times[op][0] for op in operations]\n",
    "cassandra_stds = [cassandra_times[op][1] for op in operations]\n",
    "\n",
    "# Créer un graphique pour chaque opération\n",
    "fig, axes = plt.subplots(4, 1, figsize=(10, 10))  # 4 graphiques sous le même format\n",
    "height = 0.4\n",
    "\n",
    "for i, op in enumerate(operations):\n",
    "    ax = axes[i]\n",
    "    y = np.arange(2)  # Deux barres par graphique pour SQLite et Cassandra\n",
    "\n",
    "    # Barres pour SQLite et Cassandra\n",
    "    bars1 = ax.barh(y[0] + height / 2, sqlite_times[op][0], height, xerr=sqlite_times[op][1], label='SQLite', capsize=5, color='tab:blue')\n",
    "    bars2 = ax.barh(y[1] - height / 2, cassandra_times[op][0], height, xerr=cassandra_times[op][1], label='Cassandra (1 réplicat)', capsize=5, color='tab:red')\n",
    "\n",
    "    # Titre et labels\n",
    "    ax.set_xlabel('Temps (secondes)')\n",
    "    ax.set_ylabel('Base de données')\n",
    "    ax.set_title(f'Comparaison des performances de SQLite et Cassandra pour {op}')\n",
    "    ax.set_yticks(y)\n",
    "    ax.set_yticklabels(['SQLite', 'Cassandra (1 réplicat)'])\n",
    "    ax.legend()\n",
    "\n",
    "    # Ajout des valeurs aux barres\n",
    "    def add_values(bars):\n",
    "        for bar in bars:\n",
    "            xval = bar.get_width()\n",
    "            ax.text(xval, bar.get_y() + bar.get_height() / 2, f'{xval:.6f}', va='center', ha='left')\n",
    "\n",
    "    add_values(bars1)\n",
    "    add_values(bars2)\n",
    "\n",
    "# Ajustement des graphiques pour qu'ils soient bien espacés\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valeurs de mémoire pour SQLite (current et peak)\n",
    "sqlite_memory = {\n",
    "    'CREATE': {'current': (create_sqlite_massive_mean_current_memory, create_sqlite_massive_std_current_memory),\n",
    "               'peak': (create_sqlite_massive_mean_peak_memory, create_sqlite_massive_std_peak_memory)},\n",
    "    'READ': {'current': (read_sqlite_massive_mean_current_memory, read_sqlite_massive_std_current_memory),\n",
    "             'peak': (read_sqlite_massive_mean_peak_memory, read_sqlite_massive_std_peak_memory)},\n",
    "    'UPDATE': {'current': (update_sqlite_massive_mean_current_memory, update_sqlite_massive_std_current_memory),\n",
    "               'peak': (update_sqlite_massive_mean_peak_memory, update_sqlite_massive_std_peak_memory)},\n",
    "    'DELETE': {'current': (delete_sqlite_massive_mean_current_memory, delete_sqlite_massive_std_current_memory),\n",
    "               'peak': (delete_sqlite_massive_mean_peak_memory, delete_sqlite_massive_std_peak_memory)}\n",
    "}\n",
    "\n",
    "# Valeurs de mémoire pour Cassandra (current et peak)\n",
    "cassandra_memory = {\n",
    "    'CREATE': {'current': (create_cas_massive_mean_current_memory, create_cas_massive_std_current_memory),\n",
    "               'peak': (create_cas_massive_mean_peak_memory, create_cas_massive_std_peak_memory)},\n",
    "    'READ': {'current': (read_cas_massive_mean_current_memory, read_cas_massive_std_current_memory),\n",
    "             'peak': (read_cas_massive_mean_peak_memory, read_cas_massive_std_peak_memory)},\n",
    "    'UPDATE': {'current': (update_cas_massive_mean_current_memory, update_cas_massive_std_current_memory),\n",
    "               'peak': (update_cas_massive_mean_peak_memory, update_cas_massive_std_peak_memory)},\n",
    "    'DELETE': {'current': (delete_cas_massive_mean_current_memory, delete_cas_massive_std_current_memory),\n",
    "               'peak': (delete_cas_massive_mean_peak_memory, delete_cas_massive_std_peak_memory)}\n",
    "}\n",
    "\n",
    "operations = ['CREATE', 'READ', 'UPDATE', 'DELETE']\n",
    "\n",
    "sqlite_current_means = [sqlite_memory[op]['current'][0] for op in operations]\n",
    "sqlite_current_stds = [sqlite_memory[op]['current'][1] for op in operations]\n",
    "sqlite_peak_means = [sqlite_memory[op]['peak'][0] for op in operations]\n",
    "sqlite_peak_stds = [sqlite_memory[op]['peak'][1] for op in operations]\n",
    "\n",
    "cassandra_current_means = [cassandra_memory[op]['current'][0] for op in operations]\n",
    "cassandra_current_stds = [cassandra_memory[op]['current'][1] for op in operations]\n",
    "cassandra_peak_means = [cassandra_memory[op]['peak'][0] for op in operations]\n",
    "cassandra_peak_stds = [cassandra_memory[op]['peak'][1] for op in operations]\n",
    "\n",
    "y = np.arange(len(operations)) \n",
    "height = 0.2  \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "bars_sqlite_current = ax.barh(y + 1.5 * height, sqlite_current_means, height, \n",
    "                              xerr=sqlite_current_stds, label='SQLite Current Memory', color='tab:blue', capsize=5)\n",
    "bars_cassandra_current = ax.barh(y + 0.5 * height, cassandra_current_means, height, \n",
    "                                  xerr=cassandra_current_stds, label='Cassandra Current Memory', color='tab:red', capsize=5)\n",
    "bars_sqlite_peak = ax.barh(y - 0.5 * height, sqlite_peak_means, height, \n",
    "                           xerr=sqlite_peak_stds, label='SQLite Peak Memory', color='#89CFF0', capsize=5)\n",
    "bars_cassandra_peak = ax.barh(y - 1.5 * height, cassandra_peak_means, height, \n",
    "                               xerr=cassandra_peak_stds, label='Cassandra Peak Memory', color='#FFCCBB', capsize=5)\n",
    "\n",
    "ax.set_xlabel('Mémoire utilisée (Mo)')\n",
    "ax.set_ylabel('Opérations CRUD')\n",
    "ax.set_title('Comparaison des mémoires utilisées : SQLite vs Cassandra')\n",
    "ax.set_yticks(y)\n",
    "ax.set_yticklabels(operations)\n",
    "ax.legend()\n",
    "\n",
    "def add_values(bars):\n",
    "    for bar in bars:\n",
    "        xval = bar.get_width()\n",
    "        ax.text(xval, bar.get_y() + bar.get_height() / 2, f'{xval:.2f}', va='center', ha='left')\n",
    "\n",
    "add_values(bars_sqlite_current)\n",
    "add_values(bars_cassandra_current)\n",
    "add_values(bars_sqlite_peak)\n",
    "add_values(bars_cassandra_peak)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sur un volume de données beaucoup plus important, avec toujours un seul réplicat, la requête de création (CREATE) reste la plus longue pour SQLite comme pour Cassandra.\n",
    "\n",
    "En Cassandra, les requêtes de lecture (READ) sont particulièrement rapides par rapport aux autres types de requêtes. Cela s'explique par la structure optimisée de Cassandra pour des lectures rapides, grâce à son indexation basée sur les partitions et l'utilisation de SSTables.\n",
    "\n",
    "En revanche, les requêtes de suppression (DELETE) et de mise à jour (UPDATE) sont globalement plus longues et prennent un temps similaire en Cassandra, en raison des mécanismes internes de gestion des tombstones et de la propagation des modifications.\n",
    "\n",
    "Enfin, même sur un grand volume de données, Cassandra reste globalement bien plus lent que SQLite, reflétant la différence entre une architecture distribuée conçue pour des données massives et une solution locale optimisée pour des bases de données plus légères.\n",
    "\n",
    "Idées de pourquoi Cassandra reste plus lent : \n",
    "- Cassandra distribué : Optimisé pour des bases massives, mais sa gestion des partitions et des nœuds ajoute un coût, même sur une petite base.\n",
    "- Replication : Même avec un seul réplicat, Cassandra gère la synchronisation, ce qui ralentit les écritures.\n",
    "- Caching : SQLite utilise la mémoire locale efficacement, tandis que Cassandra coordonne entre nœuds, ce qui prend plus de temps.\n",
    "\n",
    "TODO ZOE vérifier ce qui est dit ici "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6PSbYw-iawfd"
   },
   "source": [
    "## Comparaison des temps de Cassandra avec plusieurs replicaSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Qu'est ce que *SimpleStrategy* ?\n",
    "Description : Réplique les données de manière linéaire sur les n nœuds du cluster, où n est le replication_factor.\n",
    "- Avantages :\n",
    "    - Simple à configurer.\n",
    "    - Adapté aux environnements de test ou aux clusters d'une seule région.\n",
    "- Inconvénients :\n",
    "    - Pas optimisé pour les déploiements multi-régions.\n",
    "    - Répartition des réplicas pas toujours équilibrée en cas de cluster complexe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 replicas\n",
    "# --- CREATE ---\n",
    "create_cassandra_massive_mean_time_2, create_cassandra_massive_std_time_2, \\\n",
    "    create_cassandra_massive_mean_current_memory_2, create_cassandra_massive_std_current_memory_2, \\\n",
    "        create_cassandra_massive_mean_peak_memory_2, create_cassandra_massive_std_peak_memory_2 = benchmark_operation(cassandra_insert, df_initial, 2)\n",
    "\n",
    "# --- READ ---\n",
    "read_cassandra_massive_mean_time_2, read_cassandra_massive_std_time_2, \\\n",
    "    read_cassandra_massive_mean_current_memory_2, read_cassandra_massive_std_current_memory_2, \\\n",
    "        read_cassandra_massive_mean_peak_memory_2, read_cassandra_massive_std_peak_memory_2 = benchmark_operation(cassandra_read, df_initial, 2)\n",
    "\n",
    "# --- UPDATE ---\n",
    "update_cassandra_massive_mean_time_2, update_cassandra_massive_std_time_2, \\\n",
    "    update_cassandra_massive_mean_current_memory_2, update_cassandra_massive_std_current_memory_2, \\\n",
    "        update_cassandra_massive_mean_peak_memory_2, update_cassandra_massive_std_peak_memory_2 = benchmark_operation(cassandra_update, df_initial, 2)\n",
    "\n",
    "# --- DELETE ---\n",
    "delete_cassandra_massive_mean_time_2, delete_cassandra_massive_std_time_2, \\\n",
    "    delete_cassandra_massive_mean_current_memory_2, delete_cassandra_massive_std_current_memory_2, \\\n",
    "        delete_cassandra_massive_mean_peak_memory_2, delete_cassandra_massive_std_peak_memory_2 = benchmark_operation(cassandra_delete, df_initial, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 replicas\n",
    "# --- CREATE ---\n",
    "create_cassandra_massive_mean_time_5, create_cassandra_massive_std_time_5, \\\n",
    "    create_cassandra_massive_mean_current_memory_5, create_cassandra_massive_std_current_memory_5, \\\n",
    "        create_cassandra_massive_mean_peak_memory_5, create_cassandra_massive_std_peak_memory_5 = benchmark_operation(cassandra_insert, df_initial, 5)\n",
    "\n",
    "# --- READ ---\n",
    "read_cassandra_massive_mean_time_5, read_cassandra_massive_std_time_5, \\\n",
    "    read_cassandra_massive_mean_current_memory_5, read_cassandra_massive_std_current_memory_5, \\\n",
    "        read_cassandra_massive_mean_peak_memory_5, read_cassandra_massive_std_peak_memory_5 = benchmark_operation(cassandra_read, df_initial, 5)\n",
    "\n",
    "# --- UPDATE ---\n",
    "update_cassandra_massive_mean_time_5, update_cassandra_massive_std_time_5, \\\n",
    "    update_cassandra_massive_mean_current_memory_5, update_cassandra_massive_std_current_memory_5, \\\n",
    "        update_cassandra_massive_mean_peak_memory_5, update_cassandra_massive_std_peak_memory_5 = benchmark_operation(cassandra_update, df_initial, 5)\n",
    "\n",
    "# --- DELETE ---\n",
    "delete_cassandra_massive_mean_time_5, delete_cassandra_massive_std_time_5, \\\n",
    "    delete_cassandra_massive_mean_current_memory_5, delete_cassandra_massive_std_current_memory_5, \\\n",
    "        delete_cassandra_massive_mean_peak_memory_5, delete_cassandra_massive_std_peak_memory_5 = benchmark_operation(cassandra_delete, df_initial, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Données des temps pour SQLite et Cassandra\n",
    "sqlite_times = {\n",
    "    'CREATE': (create_sqlite_massive_mean_time, create_sqlite_massive_std_time),\n",
    "    'READ': (read_sqlite_massive_mean_time, read_sqlite_massive_std_time),\n",
    "    'UPDATE': (update_sqlite_massive_mean_time, update_sqlite_massive_std_time),\n",
    "    'DELETE': (delete_sqlite_massive_mean_time, delete_sqlite_massive_std_time)\n",
    "}\n",
    "\n",
    "cassandra_times = {\n",
    "    'CREATE': (create_cas_massive_mean_time, create_cas_massive_std_time),\n",
    "    'READ': (read_cas_massive_mean_time, read_cas_massive_std_time),\n",
    "    'UPDATE': (update_cas_massive_mean_time, update_cas_massive_std_time),\n",
    "    'DELETE': (delete_cas_massive_mean_time, delete_cas_massive_std_time)\n",
    "}\n",
    "\n",
    "cassandra_times_2 = {\n",
    "    'CREATE': (create_cassandra_massive_mean_time_2, create_cassandra_massive_std_time_2),\n",
    "    'READ': (read_cassandra_massive_mean_time_2, read_cassandra_massive_std_time_2),\n",
    "    'UPDATE': (update_cassandra_massive_mean_time_2, update_cassandra_massive_std_time_2),\n",
    "    'DELETE': (delete_cassandra_massive_mean_time_2, delete_cassandra_massive_std_time_2)\n",
    "}\n",
    "\n",
    "cassandra_times_5 = {\n",
    "    'CREATE': (create_cassandra_massive_mean_time_5, create_cassandra_massive_std_time_5),\n",
    "    'READ': (read_cassandra_massive_mean_time_5, read_cassandra_massive_std_time_5),\n",
    "    'UPDATE': (update_cassandra_massive_mean_time_5, update_cassandra_massive_std_time_5),\n",
    "    'DELETE': (delete_cassandra_massive_mean_time_5, delete_cassandra_massive_std_time_5)\n",
    "}\n",
    "\n",
    "operations = ['CREATE', 'READ', 'UPDATE', 'DELETE']\n",
    "\n",
    "# Extraire les moyennes et les écarts-types pour chaque opération\n",
    "sqlite_means = [sqlite_times[op][0] for op in operations]\n",
    "sqlite_stds = [sqlite_times[op][1] for op in operations]\n",
    "\n",
    "cassandra_means = [cassandra_times[op][0] for op in operations]\n",
    "cassandra_stds = [cassandra_times[op][1] for op in operations]\n",
    "\n",
    "cassandra_means_2 = [cassandra_times_2[op][0] for op in operations]\n",
    "cassandra_stds_2 = [cassandra_times_2[op][1] for op in operations]\n",
    "\n",
    "cassandra_means_5 = [cassandra_times_5[op][0] for op in operations]\n",
    "cassandra_stds_5 = [cassandra_times_5[op][1] for op in operations]\n",
    "\n",
    "# Créer un graphique pour chaque opération\n",
    "fig, axes = plt.subplots(4, 1, figsize=(10, 10))  # 4 graphiques sous le même format\n",
    "height = 0.6\n",
    "\n",
    "# Générer chaque sous-graphe pour chaque opération\n",
    "for i, op in enumerate(operations):\n",
    "    ax = axes[i]\n",
    "    y = np.arange(4)  # Quatre barres par graphique : SQLite, Cassandra (1), (2) et (5) réplicats\n",
    "\n",
    "    # Dessiner les barres pour SQLite et Cassandra avec différents réplicats\n",
    "    bars1 = ax.barh(y[0] , sqlite_times[op][0], height, xerr=sqlite_times[op][1], label='SQLite', capsize=5, color='tab:blue')\n",
    "    bars2 = ax.barh(y[1] , cassandra_times[op][0], height, xerr=cassandra_times[op][1], label='Cassandra (1 réplicat)', capsize=5, color='tab:red')\n",
    "    bars3 = ax.barh(y[2], cassandra_times_2[op][0], height, xerr=cassandra_times_2[op][1], label='Cassandra (2 réplicats)', capsize=5, color='tab:green')\n",
    "    bars4 = ax.barh(y[3], cassandra_times_5[op][0], height, xerr=cassandra_times_5[op][1], label='Cassandra (5 réplicats)', capsize=5, color='tab:orange')\n",
    "\n",
    "    # Titre et labels\n",
    "    ax.set_xlabel('Temps (secondes)')\n",
    "    ax.set_ylabel('Base de données')\n",
    "    ax.set_title(f'Comparaison des performances de SQLite et Cassandra pour {op}')\n",
    "    ax.set_yticks(y)\n",
    "    ax.set_yticklabels(['SQLite', 'Cassandra (1 réplicat)', 'Cassandra (2 réplicats)', 'Cassandra (5 réplicats)'])\n",
    "    ax.legend()\n",
    "\n",
    "    # Ajouter les valeurs aux barres\n",
    "    def add_values(bars):\n",
    "        for bar in bars:\n",
    "            xval = bar.get_width()\n",
    "            ax.text(xval, bar.get_y() + bar.get_height() / 2, f'{xval:.6f}', va='center', ha='left')\n",
    "\n",
    "    add_values(bars1)\n",
    "    add_values(bars2)\n",
    "    add_values(bars3)\n",
    "    add_values(bars4)\n",
    "\n",
    "# Ajuster l'espacement entre les graphiques\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valeurs de mémoire pour SQLite\n",
    "sqlite_memory = {\n",
    "    'CREATE': {'current': (create_sqlite_massive_mean_current_memory, create_sqlite_massive_std_current_memory),\n",
    "               'peak': (create_sqlite_massive_mean_peak_memory, create_sqlite_massive_std_peak_memory)},\n",
    "    'READ': {'current': (read_sqlite_massive_mean_current_memory, read_sqlite_massive_std_current_memory),\n",
    "             'peak': (read_sqlite_massive_mean_peak_memory, read_sqlite_massive_std_peak_memory)},\n",
    "    'UPDATE': {'current': (update_sqlite_massive_mean_current_memory, update_sqlite_massive_std_current_memory),\n",
    "               'peak': (update_sqlite_massive_mean_peak_memory, update_sqlite_massive_std_peak_memory)},\n",
    "    'DELETE': {'current': (delete_sqlite_massive_mean_current_memory, delete_sqlite_massive_std_current_memory),\n",
    "               'peak': (delete_sqlite_massive_mean_peak_memory, delete_sqlite_massive_std_peak_memory)}\n",
    "}\n",
    "\n",
    "# Valeurs de mémoire pour Cassandra avec différentes réplications\n",
    "cassandra_memory = {\n",
    "    '1-replica': {\n",
    "        'CREATE': {'current': (create_cas_massive_mean_current_memory, create_cas_massive_std_current_memory),\n",
    "                   'peak': (create_cas_massive_mean_peak_memory, create_cas_massive_std_peak_memory)},\n",
    "        'READ': {'current': (read_cas_massive_mean_current_memory, read_cas_massive_std_current_memory),\n",
    "                 'peak': (read_cas_massive_mean_peak_memory, read_cas_massive_std_peak_memory)},\n",
    "        'UPDATE': {'current': (update_cas_massive_mean_current_memory, update_cas_massive_std_current_memory),\n",
    "                   'peak': (update_cas_massive_mean_peak_memory, update_cas_massive_std_peak_memory)},\n",
    "        'DELETE': {'current': (delete_cas_massive_mean_current_memory, delete_cas_massive_std_current_memory),\n",
    "                   'peak': (delete_cas_massive_mean_peak_memory, delete_cas_massive_std_peak_memory)}\n",
    "    },\n",
    "    '2-replica': {\n",
    "        'CREATE': {'current': (create_cassandra_massive_mean_current_memory_2, create_cassandra_massive_std_current_memory_2),\n",
    "                   'peak': (create_cassandra_massive_mean_peak_memory_2, create_cassandra_massive_std_peak_memory_2)},\n",
    "        'READ': {'current': (read_cassandra_massive_mean_current_memory_2, read_cassandra_massive_std_current_memory_2),\n",
    "                 'peak': (read_cassandra_massive_mean_peak_memory_2, read_cassandra_massive_std_peak_memory_2)},\n",
    "        'UPDATE': {'current': (update_cassandra_massive_mean_current_memory_2, update_cassandra_massive_std_current_memory_2),\n",
    "                   'peak': (update_cassandra_massive_mean_peak_memory_2, update_cassandra_massive_std_peak_memory_2)},\n",
    "        'DELETE': {'current': (delete_cassandra_massive_mean_current_memory_2, delete_cassandra_massive_std_current_memory_2),\n",
    "                   'peak': (delete_cassandra_massive_mean_peak_memory_2, delete_cassandra_massive_std_peak_memory_2)}\n",
    "    },\n",
    "    '5-replica': {\n",
    "        'CREATE': {'current': (create_cassandra_massive_mean_current_memory_5, create_cassandra_massive_std_current_memory_5),\n",
    "                   'peak': (create_cassandra_massive_mean_peak_memory_5, create_cassandra_massive_std_peak_memory_5)},\n",
    "        'READ': {'current': (read_cassandra_massive_mean_current_memory_5, read_cassandra_massive_std_current_memory_5),\n",
    "                 'peak': (read_cassandra_massive_mean_peak_memory_5, read_cassandra_massive_std_peak_memory_5)},\n",
    "        'UPDATE': {'current': (update_cassandra_massive_mean_current_memory_5, update_cassandra_massive_std_current_memory_5),\n",
    "                   'peak': (update_cassandra_massive_mean_peak_memory_5, update_cassandra_massive_std_peak_memory_5)},\n",
    "        'DELETE': {'current': (delete_cassandra_massive_mean_current_memory_5, delete_cassandra_massive_std_current_memory_5),\n",
    "                   'peak': (delete_cassandra_massive_mean_peak_memory_5, delete_cassandra_massive_std_peak_memory_5)}\n",
    "    }\n",
    "}\n",
    "\n",
    "operations = ['CREATE', 'READ', 'UPDATE', 'DELETE']\n",
    "\n",
    "# Préparation des données\n",
    "sqlite_current_means = [sqlite_memory[op]['current'][0] for op in operations]\n",
    "sqlite_current_stds = [sqlite_memory[op]['current'][1] for op in operations]\n",
    "sqlite_peak_means = [sqlite_memory[op]['peak'][0] for op in operations]\n",
    "sqlite_peak_stds = [sqlite_memory[op]['peak'][1] for op in operations]\n",
    "\n",
    "cassandra_current_means = {replica: [cassandra_memory[replica][op]['current'][0] for op in operations] for replica in cassandra_memory}\n",
    "cassandra_current_stds = {replica: [cassandra_memory[replica][op]['current'][1] for op in operations] for replica in cassandra_memory}\n",
    "cassandra_peak_means = {replica: [cassandra_memory[replica][op]['peak'][0] for op in operations] for replica in cassandra_memory}\n",
    "cassandra_peak_stds = {replica: [cassandra_memory[replica][op]['peak'][1] for op in operations] for replica in cassandra_memory}\n",
    "\n",
    "y = np.arange(len(operations))\n",
    "height = 0.15\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "colors = ['tab:red', 'tab:green', 'tab:orange']\n",
    "# Current Memory\n",
    "ax[0].barh(y + 2 * height, sqlite_current_means, height, xerr=sqlite_current_stds, label='SQLite Current', color='tab:blue', capsize=5)\n",
    "for i, replica in enumerate(cassandra_memory):\n",
    "    ax[0].barh(y + (1 - i) * height, cassandra_current_means[replica], height, xerr=cassandra_current_stds[replica], \n",
    "               label=f'Cassandra {replica} Current', capsize=5, color=colors[i])\n",
    "\n",
    "\n",
    "colors = ['#FFCCCB', '#90EE90', '#FFD580']\n",
    "# Peak Memory\n",
    "ax[1].barh(y + 2 * height, sqlite_peak_means, height, xerr=sqlite_peak_stds, label='SQLite Peak', color='#89CFF0', capsize=5)\n",
    "for i, replica in enumerate(cassandra_memory):\n",
    "    ax[1].barh(y + (1 - i) * height, cassandra_peak_means[replica], height, xerr=cassandra_peak_stds[replica], \n",
    "               label=f'Cassandra {replica} Peak', capsize=5, color=colors[i])\n",
    "\n",
    "# Ajustements graphiques\n",
    "for i in range(2):\n",
    "    ax[i].set_xlabel('Mémoire utilisée (Mo)')\n",
    "    ax[i].set_ylabel('Opérations CRUD')\n",
    "    ax[i].set_yticks(y)\n",
    "    ax[i].set_yticklabels(operations)\n",
    "    ax[i].legend()\n",
    "    ax[i].grid(axis='x', linestyle='--', alpha=0.7)\n",
    "\n",
    "ax[0].set_title('Mémoire Current : SQLite vs Cassandra')\n",
    "ax[1].set_title('Mémoire Peak : SQLite vs Cassandra')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO ZOE commenter \n",
    "\n",
    "expliquer les temps : \n",
    "\n",
    "Pourquoi essayer plusieurs niveaux de réplication ?\n",
    "Impact sur les performances des lectures : Plus vous avez de répliques, plus Cassandra doit examiner plusieurs nœuds pour répondre à une requête de lecture. Cela peut augmenter la latence en fonction du niveau de réplication et de la manière dont les nœuds sont répartis.\n",
    "Impact sur les écritures : En fonction de la configuration de réplication, chaque écriture doit être propagée vers plusieurs nœuds. Cela peut augmenter le temps d'insertion ou d'update, surtout en cas de faible cohérence (si un niveau de cohérence faible est choisi).\n",
    "Tolérance aux pannes : Un nombre plus élevé de répliques permet à Cassandra de maintenir la disponibilité même en cas de panne de certains nœuds. Cependant, cela peut aussi introduire des coûts en termes de latence et de consommation de ressources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SAxurK-oSaXh"
   },
   "source": [
    "# Création d'un dataset plus grand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utiliser une base de données plus grande : Cassandra est mieux adapté pour gérer de grandes quantités de données et pour tirer parti de son architecture distribuée.\n",
    "On va donc augmenter notre jeu de données "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-BJ6a0BHSaXh"
   },
   "source": [
    "On crée nous même ce dataset car les sites de création de dataset ne permettent de télécharger que 1000 lignes avec un compte gratuit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_initial.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_0fCtTqoSaXh"
   },
   "outputs": [],
   "source": [
    "original_df = df_initial \n",
    "\n",
    "num_rows = 30000\n",
    "\n",
    "def generate_large_column(original_column, size):\n",
    "    return [random.choice(original_column) for _ in range(size)]\n",
    "\n",
    "def generate_unique_show_id(existing_ids, size):\n",
    "    start_id = max(existing_ids) + 1 if existing_ids else 1\n",
    "    return [f\"{i}\" for i in range(start_id, start_id + size)]\n",
    "\n",
    "generated_data = original_df.to_dict(orient='list')  \n",
    "\n",
    "existing_ids = set(map(int, original_df[\"show_id\"].tolist())) \n",
    "new_ids = generate_unique_show_id(existing_ids, num_rows - len(original_df))\n",
    "generated_data[\"show_id\"].extend(new_ids)\n",
    "\n",
    "for column in original_df.columns:\n",
    "    if column != \"show_id\":  \n",
    "        generated_data[column].extend(generate_large_column(original_df[column].tolist(), num_rows - len(original_df)))\n",
    "\n",
    "large_df = pd.DataFrame(generated_data)\n",
    "\n",
    "large_df = large_df.drop_duplicates()\n",
    "\n",
    "for column in large_df.columns:\n",
    "    if large_df[column].isnull().any():\n",
    "        if large_df[column].dtype == \"object\":\n",
    "            large_df[column].fillna(\"Unknown\", inplace=True)  \n",
    "        else:\n",
    "            large_df[column].fillna(0, inplace=True)  \n",
    "\n",
    "large_df = large_df.drop_duplicates()\n",
    "\n",
    "if large_df[\"show_id\"].duplicated().any():\n",
    "    print(\"Attention : Des doublons existent dans la colonne 'show_id'.\")\n",
    "else:\n",
    "    print(\"Les IDs sont uniques.\")\n",
    "\n",
    "output_file = \"netflix_titles_large.csv\"\n",
    "large_df.to_csv(output_file, index=False)\n",
    "print(f\"Dataset généré avec {len(large_df)} lignes (lignes dupliquées supprimées) et exporté dans {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caster correctement les colonnes \n",
    "large_df['show_id'] = large_df['show_id'].astype(int)\n",
    "\n",
    "large_df['show_id'] = large_df['show_id'].astype(int)\n",
    "large_df['release_year'] = large_df['release_year'].astype(int)\n",
    "\n",
    "large_df['type'] = large_df['title'].astype(str)\n",
    "large_df['title'] = large_df['title'].astype(str)\n",
    "large_df['director'] = large_df['director'].astype(str)\n",
    "large_df['cast'] = large_df['cast'].astype(str)\n",
    "large_df['country'] = large_df['country'].astype(str)\n",
    "large_df['date_added'] = large_df['date_added'].astype(str)\n",
    "large_df['rating'] = large_df['rating'].astype(str)\n",
    "large_df['duration'] = large_df['duration'].astype(str)\n",
    "large_df['listed_in'] = large_df['listed_in'].astype(str)\n",
    "large_df['description'] = large_df['description'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sqlite \n",
    "# --- CREATE ---\n",
    "create_sqlite_massive_mean_time, create_sqlite_massive_std_time, \\\n",
    "    create_sqlite_massive_mean_current_memory, create_sqlite_massive_std_current_memory, \\\n",
    "        create_sqlite_massive_mean_peak_memory, create_sqlite_massive_std_peak_memory = benchmark_operation(sqlite_insert, large_df)\n",
    "\n",
    "# --- READ ---\n",
    "read_sqlite_massive_mean_time, read_sqlite_massive_std_time, \\\n",
    "    read_sqlite_massive_mean_current_memory, read_sqlite_massive_std_current_memory, \\\n",
    "        read_sqlite_massive_mean_peak_memory, read_sqlite_massive_std_peak_memory = benchmark_operation(sqlite_read, large_df)\n",
    "\n",
    "# --- UPDATE ---\n",
    "update_sqlite_massive_mean_time, update_sqlite_massive_std_time, \\\n",
    "    update_sqlite_massive_mean_current_memory, update_sqlite_massive_std_current_memory, \\\n",
    "        update_sqlite_massive_mean_peak_memory, update_sqlite_massive_std_peak_memory = benchmark_operation(sqlite_update, large_df)\n",
    "\n",
    "# --- DELETE ---\n",
    "delete_sqlite_massive_mean_time, delete_sqlite_massive_std_time, \\\n",
    "    delete_sqlite_massive_mean_current_memory, delete_sqlite_massive_std_current_memory, \\\n",
    "        delete_sqlite_massive_mean_peak_memory, delete_sqlite_massive_std_peak_memory = benchmark_operation(sqlite_delete, large_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cassandra\n",
    "\n",
    "# 1 replica\n",
    "# --- CREATE ---\n",
    "create_cas_mean_time, create_cas_std_time, \\\n",
    "    create_cas_mean_current_memory, create_cas_std_current_memory, \\\n",
    "        create_cas_mean_peak_memory, create_cas_std_peak_memory = benchmark_operation(cassandra_insert, large_df, 1)\n",
    "\n",
    "# --- READ ---\n",
    "read_cas_mean_time, read_cas_std_time, \\\n",
    "    read_cas_mean_current_memory, read_cas_std_current_memory, \\\n",
    "        read_cas_mean_peak_memory, read_cas_std_peak_memory = benchmark_operation(cassandra_read, large_df, 1)\n",
    "\n",
    "# --- UPDATE ---\n",
    "update_cas_mean_time, update_cas_std_time, \\\n",
    "    update_cas_mean_current_memory, update_cas_std_current_memory, \\\n",
    "        update_cas_mean_peak_memory, update_cas_std_peak_memory = benchmark_operation(cassandra_update, large_df, 1)\n",
    "\n",
    "# --- DELETE ---\n",
    "delete_cas_mean_time, delete_cas_std_time, \\\n",
    "    delete_cas_mean_current_memory, delete_cas_std_current_memory, \\\n",
    "        delete_cas_mean_peak_memory, delete_cas_std_peak_memory = benchmark_operation(cassandra_delete, large_df, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 replicas\n",
    "# --- CREATE ---\n",
    "create_cassandra_massive_mean_time_2, create_cassandra_massive_std_time_2, \\\n",
    "    create_cassandra_massive_mean_current_memory_2, create_cassandra_massive_std_current_memory_2, \\\n",
    "        create_cassandra_massive_mean_peak_memory_2, create_cassandra_massive_std_peak_memory_2 = benchmark_operation(cassandra_insert, large_df, 2)\n",
    "\n",
    "# --- READ ---\n",
    "read_cassandra_massive_mean_time_2, read_cassandra_massive_std_time_2, \\\n",
    "    read_cassandra_massive_mean_current_memory_2, read_cassandra_massive_std_current_memory_2, \\\n",
    "        read_cassandra_massive_mean_peak_memory_2, read_cassandra_massive_std_peak_memory_2 = benchmark_operation(cassandra_read, large_df, 2)\n",
    "\n",
    "# --- UPDATE ---\n",
    "update_cassandra_massive_mean_time_2, update_cassandra_massive_std_time_2, \\\n",
    "    update_cassandra_massive_mean_current_memory_2, update_cassandra_massive_std_current_memory_2, \\\n",
    "        update_cassandra_massive_mean_peak_memory_2, update_cassandra_massive_std_peak_memory_2 = benchmark_operation(cassandra_update, large_df, 2)\n",
    "\n",
    "# --- DELETE ---\n",
    "delete_cassandra_massive_mean_time_2, delete_cassandra_massive_std_time_2, \\\n",
    "    delete_cassandra_massive_mean_current_memory_2, delete_cassandra_massive_std_current_memory_2, \\\n",
    "        delete_cassandra_massive_mean_peak_memory_2, delete_cassandra_massive_std_peak_memory_2 = benchmark_operation(cassandra_delete, large_df, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 replicas\n",
    "# --- CREATE ---\n",
    "create_cassandra_massive_mean_time_5, create_cassandra_massive_std_time_5, \\\n",
    "    create_cassandra_massive_mean_current_memory_5, create_cassandra_massive_std_current_memory_5, \\\n",
    "        create_cassandra_massive_mean_peak_memory_5, create_cassandra_massive_std_peak_memory_5 = benchmark_operation(cassandra_insert, large_df, 5)\n",
    "\n",
    "# --- READ ---\n",
    "read_cassandra_massive_mean_time_5, read_cassandra_massive_std_time_5, \\\n",
    "    read_cassandra_massive_mean_current_memory_5, read_cassandra_massive_std_current_memory_5, \\\n",
    "        read_cassandra_massive_mean_peak_memory_5, read_cassandra_massive_std_peak_memory_5 = benchmark_operation(cassandra_read, large_df, 5)\n",
    "\n",
    "# --- UPDATE ---\n",
    "update_cassandra_massive_mean_time_5, update_cassandra_massive_std_time_5, \\\n",
    "    update_cassandra_massive_mean_current_memory_5, update_cassandra_massive_std_current_memory_5, \\\n",
    "        update_cassandra_massive_mean_peak_memory_5, update_cassandra_massive_std_peak_memory_5 = benchmark_operation(cassandra_update, large_df, 5)\n",
    "\n",
    "# --- DELETE ---\n",
    "delete_cassandra_massive_mean_time_5, delete_cassandra_massive_std_time_5, \\\n",
    "    delete_cassandra_massive_mean_current_memory_5, delete_cassandra_massive_std_current_memory_5, \\\n",
    "        delete_cassandra_massive_mean_peak_memory_5, delete_cassandra_massive_std_peak_memory_5 = benchmark_operation(cassandra_delete, large_df, 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlite_times = {\n",
    "    'CREATE': (create_sqlite_massive_mean_time, create_sqlite_massive_std_time),\n",
    "    'READ': (read_sqlite_massive_mean_time, read_sqlite_massive_std_time),\n",
    "    'UPDATE': (update_sqlite_massive_mean_time, update_sqlite_massive_std_time),\n",
    "    'DELETE': (delete_sqlite_massive_mean_time, delete_sqlite_massive_std_time)\n",
    "}\n",
    "\n",
    "cassandra_times = {\n",
    "    'CREATE': (create_cas_massive_mean_time, create_cas_massive_std_time),\n",
    "    'READ': (read_cas_massive_mean_time, read_cas_massive_std_time),\n",
    "    'UPDATE': (update_cas_massive_mean_time, update_cas_massive_std_time),\n",
    "    'DELETE': (delete_cas_massive_mean_time, delete_cas_massive_std_time)\n",
    "}\n",
    "\n",
    "cassandra_times_2 = {\n",
    "    'CREATE': (create_cassandra_massive_mean_time_2, create_cassandra_massive_std_time_2),\n",
    "    'READ': (read_cassandra_massive_mean_time_2, read_cassandra_massive_std_time_2),\n",
    "    'UPDATE': (update_cassandra_massive_mean_time_2, update_cassandra_massive_std_time_2),\n",
    "    'DELETE': (delete_cassandra_massive_mean_time_2, delete_cassandra_massive_std_time_2)\n",
    "}\n",
    "\n",
    "cassandra_times_5 = {\n",
    "    'CREATE': (create_cassandra_massive_mean_time_5, create_cassandra_massive_std_time_5),\n",
    "    'READ': (read_cassandra_massive_mean_time_5, read_cassandra_massive_std_time_5),\n",
    "    'UPDATE': (update_cassandra_massive_mean_time_5, update_cassandra_massive_std_time_5),\n",
    "    'DELETE': (delete_cassandra_massive_mean_time_5, delete_cassandra_massive_std_time_5)\n",
    "}\n",
    "\n",
    "operations = ['CREATE', 'READ', 'UPDATE', 'DELETE']\n",
    "\n",
    "# Extraire les temps moyens et écarts-types pour chaque système\n",
    "sqlite_means = [sqlite_times[op][0] for op in operations]\n",
    "sqlite_stds = [sqlite_times[op][1] for op in operations]\n",
    "\n",
    "cassandra_means = [cassandra_times[op][0] for op in operations]\n",
    "cassandra_stds = [cassandra_times[op][1] for op in operations]\n",
    "\n",
    "cassandra_means_2 = [cassandra_times_2[op][0] for op in operations]\n",
    "cassandra_stds_2 = [cassandra_times_2[op][1] for op in operations]\n",
    "\n",
    "cassandra_means_5 = [cassandra_times_5[op][0] for op in operations]\n",
    "cassandra_stds_5 = [cassandra_times_5[op][1] for op in operations]\n",
    "\n",
    "# Créer un graphique pour chaque opération\n",
    "fig, axes = plt.subplots(4, 1, figsize=(10, 10))  # 4 graphiques sous le même format\n",
    "height = 0.6\n",
    "\n",
    "# Générer chaque sous-graphe pour chaque opération\n",
    "for i, op in enumerate(operations):\n",
    "    ax = axes[i]\n",
    "    y = np.arange(4)  # Quatre barres par graphique : SQLite, Cassandra (1), (2) et (5) réplicats\n",
    "\n",
    "    # Dessiner les barres pour SQLite et Cassandra avec différents réplicats\n",
    "    bars1 = ax.barh(y[0] , sqlite_times[op][0], height, xerr=sqlite_times[op][1], label='SQLite', capsize=5, color='tab:blue')\n",
    "    bars2 = ax.barh(y[1] , cassandra_times[op][0], height, xerr=cassandra_times[op][1], label='Cassandra (1 réplicat)', capsize=5, color='tab:red')\n",
    "    bars3 = ax.barh(y[2], cassandra_times_2[op][0], height, xerr=cassandra_times_2[op][1], label='Cassandra (2 réplicats)', capsize=5, color='tab:green')\n",
    "    bars4 = ax.barh(y[3], cassandra_times_5[op][0], height, xerr=cassandra_times_5[op][1], label='Cassandra (5 réplicats)', capsize=5, color='tab:orange')\n",
    "\n",
    "    # Titre et labels\n",
    "    ax.set_xlabel('Temps (secondes)')\n",
    "    ax.set_ylabel('Base de données')\n",
    "    ax.set_title(f'Comparaison des performances de SQLite et Cassandra pour {op}')\n",
    "    ax.set_yticks(y)\n",
    "    ax.set_yticklabels(['SQLite', 'Cassandra (1 réplicat)', 'Cassandra (2 réplicats)', 'Cassandra (5 réplicats)'])\n",
    "    ax.legend()\n",
    "\n",
    "    # Ajouter les valeurs aux barres\n",
    "    def add_values(bars):\n",
    "        for bar in bars:\n",
    "            xval = bar.get_width()\n",
    "            ax.text(xval, bar.get_y() + bar.get_height() / 2, f'{xval:.6f}', va='center', ha='left')\n",
    "\n",
    "    add_values(bars1)\n",
    "    add_values(bars2)\n",
    "    add_values(bars3)\n",
    "    add_values(bars4)\n",
    "\n",
    "# Ajuster l'espacement entre les graphiques\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Valeurs de mémoire pour SQLite\n",
    "sqlite_memory = {\n",
    "    'CREATE': {'current': (create_sqlite_massive_mean_current_memory, create_sqlite_massive_std_current_memory),\n",
    "               'peak': (create_sqlite_massive_mean_peak_memory, create_sqlite_massive_std_peak_memory)},\n",
    "    'READ': {'current': (read_sqlite_massive_mean_current_memory, read_sqlite_massive_std_current_memory),\n",
    "             'peak': (read_sqlite_massive_mean_peak_memory, read_sqlite_massive_std_peak_memory)},\n",
    "    'UPDATE': {'current': (update_sqlite_massive_mean_current_memory, update_sqlite_massive_std_current_memory),\n",
    "               'peak': (update_sqlite_massive_mean_peak_memory, update_sqlite_massive_std_peak_memory)},\n",
    "    'DELETE': {'current': (delete_sqlite_massive_mean_current_memory, delete_sqlite_massive_std_current_memory),\n",
    "               'peak': (delete_sqlite_massive_mean_peak_memory, delete_sqlite_massive_std_peak_memory)}\n",
    "}\n",
    "\n",
    "# Valeurs de mémoire pour Cassandra avec différentes réplications\n",
    "cassandra_memory = {\n",
    "    '1-replica': {\n",
    "        'CREATE': {'current': (create_cas_massive_mean_current_memory, create_cas_massive_std_current_memory),\n",
    "                   'peak': (create_cas_massive_mean_peak_memory, create_cas_massive_std_peak_memory)},\n",
    "        'READ': {'current': (read_cas_massive_mean_current_memory, read_cas_massive_std_current_memory),\n",
    "                 'peak': (read_cas_massive_mean_peak_memory, read_cas_massive_std_peak_memory)},\n",
    "        'UPDATE': {'current': (update_cas_massive_mean_current_memory, update_cas_massive_std_current_memory),\n",
    "                   'peak': (update_cas_massive_mean_peak_memory, update_cas_massive_std_peak_memory)},\n",
    "        'DELETE': {'current': (delete_cas_massive_mean_current_memory, delete_cas_massive_std_current_memory),\n",
    "                   'peak': (delete_cas_massive_mean_peak_memory, delete_cas_massive_std_peak_memory)}\n",
    "    },\n",
    "    '2-replica': {\n",
    "        'CREATE': {'current': (create_cassandra_massive_mean_current_memory_2, create_cassandra_massive_std_current_memory_2),\n",
    "                   'peak': (create_cassandra_massive_mean_peak_memory_2, create_cassandra_massive_std_peak_memory_2)},\n",
    "        'READ': {'current': (read_cassandra_massive_mean_current_memory_2, read_cassandra_massive_std_current_memory_2),\n",
    "                 'peak': (read_cassandra_massive_mean_peak_memory_2, read_cassandra_massive_std_peak_memory_2)},\n",
    "        'UPDATE': {'current': (update_cassandra_massive_mean_current_memory_2, update_cassandra_massive_std_current_memory_2),\n",
    "                   'peak': (update_cassandra_massive_mean_peak_memory_2, update_cassandra_massive_std_peak_memory_2)},\n",
    "        'DELETE': {'current': (delete_cassandra_massive_mean_current_memory_2, delete_cassandra_massive_std_current_memory_2),\n",
    "                   'peak': (delete_cassandra_massive_mean_peak_memory_2, delete_cassandra_massive_std_peak_memory_2)}\n",
    "    },\n",
    "    '5-replica': {\n",
    "        'CREATE': {'current': (create_cassandra_massive_mean_current_memory_5, create_cassandra_massive_std_current_memory_5),\n",
    "                   'peak': (create_cassandra_massive_mean_peak_memory_5, create_cassandra_massive_std_peak_memory_5)},\n",
    "        'READ': {'current': (read_cassandra_massive_mean_current_memory_5, read_cassandra_massive_std_current_memory_5),\n",
    "                 'peak': (read_cassandra_massive_mean_peak_memory_5, read_cassandra_massive_std_peak_memory_5)},\n",
    "        'UPDATE': {'current': (update_cassandra_massive_mean_current_memory_5, update_cassandra_massive_std_current_memory_5),\n",
    "                   'peak': (update_cassandra_massive_mean_peak_memory_5, update_cassandra_massive_std_peak_memory_5)},\n",
    "        'DELETE': {'current': (delete_cassandra_massive_mean_current_memory_5, delete_cassandra_massive_std_current_memory_5),\n",
    "                   'peak': (delete_cassandra_massive_mean_peak_memory_5, delete_cassandra_massive_std_peak_memory_5)}\n",
    "    }\n",
    "}\n",
    "\n",
    "operations = ['CREATE', 'READ', 'UPDATE', 'DELETE']\n",
    "\n",
    "# Préparation des données\n",
    "sqlite_current_means = [sqlite_memory[op]['current'][0] for op in operations]\n",
    "sqlite_current_stds = [sqlite_memory[op]['current'][1] for op in operations]\n",
    "sqlite_peak_means = [sqlite_memory[op]['peak'][0] for op in operations]\n",
    "sqlite_peak_stds = [sqlite_memory[op]['peak'][1] for op in operations]\n",
    "\n",
    "cassandra_current_means = {replica: [cassandra_memory[replica][op]['current'][0] for op in operations] for replica in cassandra_memory}\n",
    "cassandra_current_stds = {replica: [cassandra_memory[replica][op]['current'][1] for op in operations] for replica in cassandra_memory}\n",
    "cassandra_peak_means = {replica: [cassandra_memory[replica][op]['peak'][0] for op in operations] for replica in cassandra_memory}\n",
    "cassandra_peak_stds = {replica: [cassandra_memory[replica][op]['peak'][1] for op in operations] for replica in cassandra_memory}\n",
    "\n",
    "y = np.arange(len(operations))\n",
    "height = 0.15\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "colors = ['tab:red', 'tab:green', 'tab:orange']\n",
    "# Current Memory\n",
    "ax[0].barh(y + 2 * height, sqlite_current_means, height, xerr=sqlite_current_stds, label='SQLite Current', color='tab:blue', capsize=5)\n",
    "for i, replica in enumerate(cassandra_memory):\n",
    "    ax[0].barh(y + (1 - i) * height, cassandra_current_means[replica], height, xerr=cassandra_current_stds[replica], \n",
    "               label=f'Cassandra {replica} Current', capsize=5, color=colors[i])\n",
    "\n",
    "\n",
    "colors = ['#FFCCCB', '#90EE90', '#FFD580']\n",
    "# Peak Memory\n",
    "ax[1].barh(y + 2 * height, sqlite_peak_means, height, xerr=sqlite_peak_stds, label='SQLite Peak', color='#89CFF0', capsize=5)\n",
    "for i, replica in enumerate(cassandra_memory):\n",
    "    ax[1].barh(y + (1 - i) * height, cassandra_peak_means[replica], height, xerr=cassandra_peak_stds[replica], \n",
    "               label=f'Cassandra {replica} Peak', capsize=5, color=colors[i])\n",
    "\n",
    "# Ajustements graphiques\n",
    "for i in range(2):\n",
    "    ax[i].set_xlabel('Mémoire utilisée (Mo)')\n",
    "    ax[i].set_ylabel('Opérations CRUD')\n",
    "    ax[i].set_yticks(y)\n",
    "    ax[i].set_yticklabels(operations)\n",
    "    ax[i].legend()\n",
    "    ax[i].grid(axis='x', linestyle='--', alpha=0.7)\n",
    "\n",
    "ax[0].set_title('Mémoire Current : SQLite vs Cassandra')\n",
    "ax[1].set_title('Mémoire Peak : SQLite vs Cassandra')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO ZOE commenter ZOE la visualisation aussi "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparaison avec / sans index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index secondaires (clés secondaires)\n",
    "\n",
    "\n",
    "Dans Cassandra, la commande `CREATE INDEX` permet de créer des index secondaires, qui ne sont pas des clés primaires. Cela permet de filtrer les données sur des colonnes autres que la partition key ou les clustering keys.\n",
    "\n",
    "#### Avantages des index secondaires\n",
    "\n",
    "- Facilite les filtres sur des colonnes non clés : Vous pouvez filtrer sur des colonnes autres que la partition key ou les clustering keys, ce qui rend les requêtes plus flexibles.\n",
    "- Accès plus rapide aux données : Les index secondaires optimisent les requêtes qui filtrent sur certaines colonnes spécifiques, réduisant le nombre de partitions à examiner.\n",
    "\n",
    "#### Inconvénients des index secondaires\n",
    "\n",
    "- Risque de surcharge de performance : Sur de grandes tables, si une colonne contient beaucoup de valeurs distinctes, l'index peut devenir lourd et ralentir les performances des écritures et lectures.\n",
    "- Structure distribuée moins efficace : Les index secondaires sont distribués, mais leur gestion est moins performante que les clés primaires, surtout sur des colonnes avec une haute cardinalité.\n",
    "- Limitations d'usage : Les index secondaires sont mieux utilisés sur des colonnes avec faible cardinalité. Il est déconseillé de les utiliser pour des colonnes ayant une forte cardinalité ou pour des filtres complexes impliquant plusieurs colonnes.\n",
    "- `ALLOW FILTERING` : Parfois, pour exécuter une requête avec un index secondaire (utilisation de `<` ou `>` par exemple), il est obligatoire d'utiliser `ALLOW FILTERING`, ce qui peut nuire aux performances en scannant une grande partie des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cassandra_create_indexes(columns):\n",
    "    tracemalloc.start()\n",
    "    start_time = time.time()\n",
    "    for column in columns:\n",
    "        index_query = f\"CREATE INDEX {column}_idx ON shows ({column});\"\n",
    "        session.execute(index_query)\n",
    "    cassandra_time = time.time() - start_time\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    current = current / 1024 / 1024\n",
    "    peak = peak / 1024 / 1024\n",
    "    tracemalloc.stop()\n",
    "    return cassandra_time, current, peak\n",
    "\n",
    "\n",
    "def sqlite_create_indexes(columns):\n",
    "    # drop les index\n",
    "    for column in columns:\n",
    "        index_query = f\"DROP INDEX IF EXISTS {column}_idx;\"\n",
    "        cursor.execute(index_query)\n",
    "\n",
    "    tracemalloc.start()\n",
    "    start_time = time.time()\n",
    "    for column in columns:\n",
    "        # Créer une requête SQL pour chaque colonne\n",
    "        index_query = f\"CREATE INDEX IF NOT EXISTS {column}_idx ON shows ({column});\"\n",
    "        cursor.execute(index_query)\n",
    "    sqlite_time = time.time() - start_time\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    current = current / 1024 / 1024\n",
    "    peak = peak / 1024 / 1024\n",
    "    tracemalloc.stop()\n",
    "    return sqlite_time, current, peak\n",
    "\n",
    "columns_to_index = [\"release_year\"]\n",
    "cassandra_time_index, cassandra_current_index, cassandra_peak_index = cassandra_create_indexes(columns_to_index)\n",
    "sqlite_time_index, sqlite_current_index, sqlite_peak_index = sqlite_create_indexes(columns_to_index)\n",
    "\n",
    "print(f\"Indexation time: Cassandra {cassandra_time_index:.5f}s, SQLite {sqlite_time_index:.5f}s\")\n",
    "print(f\"Current memory: Cassandra {cassandra_current_index:.5f}Mo, SQLite {sqlite_current_index:.5f}Mo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonctions pour CRUD SQLite avec index\n",
    "\n",
    "# --- CREATE ---\n",
    "def sqlite_insert_index(df):\n",
    "    drop_table_sqlite()\n",
    "    create_table_sqlite()\n",
    "    sqlite_create_indexes(columns_to_index)\n",
    "\n",
    "    columns = ', '.join(df.columns)\n",
    "    placeholders = ', '.join(['?'] * len(df.columns))\n",
    "    insert_query = f\"INSERT INTO shows ({columns}) VALUES ({placeholders})\"\n",
    "\n",
    "    tracemalloc.start()\n",
    "    start_time = time.time()\n",
    "    for _, row in df.iterrows():\n",
    "      cursor.execute(insert_query, tuple(row))\n",
    "    conn.commit()\n",
    "\n",
    "    create_time_sqlite = time.time() - start_time\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    current = current / 1024 / 1024\n",
    "    peak = peak / 1024 / 1024\n",
    "    tracemalloc.stop()\n",
    "    print(f\"Create Time: {create_time_sqlite:.6f} seconds\")\n",
    "    print(f\"Current memory usage is {current}Mo; Peak was {peak}Mo\")\n",
    "    return create_time_sqlite, current, peak\n",
    "\n",
    "# --- READ ---\n",
    "def sqlite_read_index(df):\n",
    "    sqlite_insert_index(df)\n",
    "    tracemalloc.start()\n",
    "    start_time = time.time()\n",
    "    cursor.execute(\"SELECT * FROM shows WHERE release_year = 2000\")\n",
    "    result = cursor.fetchall()\n",
    "    read_time_sqlite = time.time() - start_time\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    current = current / 1024 / 1024\n",
    "    peak = peak / 1024 / 1024\n",
    "    tracemalloc.stop()\n",
    "    print(f\"Read Time: {read_time_sqlite:.6f} seconds\")\n",
    "    print(f\"Queried {len(result)} records\")\n",
    "    print(f\"Current memory usage is {current}Mo; Peak was {peak}Mo\")\n",
    "    return read_time_sqlite, current, peak\n",
    "\n",
    "# --- UPDATE ---\n",
    "def sqlite_update_index(df):\n",
    "    sqlite_insert_index(df)\n",
    "    tracemalloc.start()\n",
    "    start_time = time.time()\n",
    "    cursor.execute(\"UPDATE shows SET rating='PG' WHERE release_year = 2000\")\n",
    "    conn.commit()\n",
    "    update_time_sqlite = time.time() - start_time\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    current = current / 1024 / 1024\n",
    "    peak = peak / 1024 / 1024\n",
    "    tracemalloc.stop()\n",
    "    print(f\"Update Time: {update_time_sqlite:.6f} seconds\")\n",
    "    print(f\"Current memory usage is {current}Mo; Peak was {peak}Mo\")\n",
    "    return update_time_sqlite, current, peak\n",
    "\n",
    "# --- DELETE ---\n",
    "def sqlite_delete_index(df):\n",
    "    sqlite_insert_index(df)\n",
    "    tracemalloc.start()\n",
    "    start_time = time.time()\n",
    "    cursor.execute(\"DELETE FROM shows WHERE release_year = 2000\")\n",
    "    conn.commit()\n",
    "    delete_time_sqlite = time.time() - start_time\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    current = current / 1024 / 1024\n",
    "    peak = peak / 1024 / 1024\n",
    "    tracemalloc.stop()\n",
    "    print(f\"Delete Time: {delete_time_sqlite:.6f} seconds\")\n",
    "    print(f\"Current memory usage is {current}Mo; Peak was {peak}Mo\")\n",
    "    return delete_time_sqlite, current, peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonctions pour CRUD Cassandra avec index\n",
    "\n",
    "# --- CREATE ---\n",
    "def cassandra_insert_index(*args):\n",
    "    df = args[0]  # Premier argument, qui est le DataFrame\n",
    "    nb_repl = args[1]  # Deuxième argument, qui est le nombre de répliques\n",
    "\n",
    "    drop_table_cassandra()\n",
    "    create_table_cassandra(nb_repl)\n",
    "    cassandra_create_indexes(columns_to_index)\n",
    "\n",
    "    insert_query = session.prepare(\"INSERT INTO shows (show_id, title, director, cast, country, date_added, release_year, rating, duration, listed_in, description) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\")\n",
    "\n",
    "    tracemalloc.start()\n",
    "    start_time = time.time()\n",
    "    for _, row in df.iterrows():\n",
    "        session.execute(insert_query, (row['show_id'], row['title'], row['director'], row['cast'], row['country'], row['date_added'], row['release_year'], row['rating'], row['duration'], row['listed_in'], row['description']))\n",
    "    create_time = time.time() - start_time\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    current = current / 1024 / 1024\n",
    "    peak = peak / 1024 / 1024\n",
    "    tracemalloc.stop()\n",
    "\n",
    "    print(f\"Create Time: {create_time:.6f} seconds\")\n",
    "    print(f\"Current memory usage is {current}Mo; Peak was {peak}Mo\")\n",
    "\n",
    "    return create_time, current, peak\n",
    "\n",
    "\n",
    "# --- READ ---\n",
    "def cassandra_read_index(*args):\n",
    "    df = args[0]  # Premier argument, qui est le DataFrame\n",
    "    nb_repl = args[1]  # Deuxième argument, qui est le nombre de répliques\n",
    "    cassandra_insert_index(df, nb_repl)\n",
    "\n",
    "    tracemalloc.start()\n",
    "    start_time = time.time()\n",
    "    result = session.execute('SELECT * FROM shows WHERE release_year = 2000 ;') # ici pas de allow filtering !\n",
    "    read_time = time.time() - start_time\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    current = current / 1024 / 1024\n",
    "    peak = peak / 1024 / 1024\n",
    "    tracemalloc.stop()\n",
    "    print(f\"Read time with index: {read_time:.5f}s\")\n",
    "    num_records = sum(1 for _ in result)\n",
    "    print(f\"Number of records: {num_records}\")\n",
    "    print(f\"Current memory usage is {current}Mo; Peak was {peak}Mo\")\n",
    "    return read_time, current, peak\n",
    "\n",
    "# Dans Cassandra, pour effectuer un UPDATE, il est nécessaire de spécifier la clé primaire complète (partition key + clustering key).\n",
    "# Par conséquent, il n'est pas possible de mettre à jour des lignes en fonction de critères de recherche arbitraires, sauf si ces critères correspondent exactement à la clé primaire.\n",
    "\n",
    "# --- UPDATE ---\n",
    "def cassandra_update_index(*args):\n",
    "    df = args[0]  # Premier argument, qui est le DataFrame\n",
    "    nb_repl = args[1]  # Deuxième argument, qui est le nombre de répliques\n",
    "    cassandra_insert_index(df, nb_repl)\n",
    "\n",
    "    tracemalloc.start()\n",
    "    start_time = time.time()\n",
    "    select_query = \"SELECT show_id FROM shows WHERE release_year = 2000;\"\n",
    "    result = session.execute(select_query)\n",
    "    show_ids_to_update = [row.show_id for row in result]\n",
    "    for show_id in show_ids_to_update:\n",
    "        update_query = f\"UPDATE shows SET rating = 'PG' WHERE show_id = {show_id};\"\n",
    "        session.execute(update_query)\n",
    "    update_time = time.time() - start_time\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    current = current / 1024 / 1024\n",
    "    peak = peak / 1024 / 1024\n",
    "    tracemalloc.stop()\n",
    "    print(f\"Update time with index: {update_time:.5f}s\")\n",
    "    print(f\"Current memory usage is {current}Mo; Peak was {peak}Mo\")\n",
    "    return update_time, current, peak\n",
    "\n",
    "\n",
    "# --- DELETE ---\n",
    "def cassandra_delete_index(*args):\n",
    "    df = args[0]  # Premier argument, qui est le DataFrame\n",
    "    nb_repl = args[1]  # Deuxième argument, qui est le nombre de répliques\n",
    "    cassandra_insert_index(df, nb_repl)\n",
    "\n",
    "    tracemalloc.start()\n",
    "    start_time = time.time()\n",
    "    select_query = \"SELECT show_id FROM shows WHERE release_year = 2000;\"\n",
    "    result = session.execute(select_query)\n",
    "    show_ids_to_delete = [row.show_id for row in result]\n",
    "    for show_id in show_ids_to_delete:\n",
    "        delete_query = f\"DELETE FROM shows WHERE show_id = {show_id};\"\n",
    "        session.execute(delete_query)\n",
    "    delete_time = time.time() - start_time\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    current = current / 1024 / 1024\n",
    "    peak = peak / 1024 / 1024\n",
    "    tracemalloc.stop()\n",
    "    print(f\"Delete time with index: {delete_time:.5f}s\")\n",
    "    print(f\"Current memory usage is {current}Mo; Peak was {peak}Mo\")\n",
    "    return delete_time, current, peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sqlite \n",
    "# --- CREATE ---\n",
    "create_sqlite_massive_mean_time_index, create_sqlite_massive_std_time_index, \\\n",
    "    create_sqlite_massive_mean_current_memory_index, create_sqlite_massive_std_current_memory_index, \\\n",
    "        create_sqlite_massive_mean_peak_memory_index, create_sqlite_massive_std_peak_memory_index = benchmark_operation(sqlite_insert_index, large_df)\n",
    "\n",
    "# --- READ ---\n",
    "read_sqlite_massive_mean_time_index, read_sqlite_massive_std_time_index, \\\n",
    "    read_sqlite_massive_mean_current_memory_index, read_sqlite_massive_std_current_memory_index, \\\n",
    "        read_sqlite_massive_mean_peak_memory_index, read_sqlite_massive_std_peak_memory_index = benchmark_operation(sqlite_read_index, large_df)\n",
    "\n",
    "# --- UPDATE ---\n",
    "update_sqlite_massive_mean_time_index, update_sqlite_massive_std_time_index, \\\n",
    "    update_sqlite_massive_mean_current_memory_index, update_sqlite_massive_std_current_memory_index, \\\n",
    "        update_sqlite_massive_mean_peak_memory_index, update_sqlite_massive_std_peak_memory_index = benchmark_operation(sqlite_update_index, large_df)\n",
    "\n",
    "# --- DELETE ---\n",
    "delete_sqlite_massive_mean_time_index, delete_sqlite_massive_std_time_index, \\\n",
    "    delete_sqlite_massive_mean_current_memory_index, delete_sqlite_massive_std_current_memory_index, \\\n",
    "        delete_sqlite_massive_mean_peak_memory_index, delete_sqlite_massive_std_peak_memory_index = benchmark_operation(sqlite_delete_index, large_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cassandra\n",
    "\n",
    "# 1 replica\n",
    "# --- CREATE ---\n",
    "create_cassandra_mean_time_index, create_cassandra_std_time_index, \\\n",
    "    create_cassandra_mean_current_memory_index, create_cassandra_std_current_memory_index, \\\n",
    "        create_cassandra_mean_peak_memory_index, create_cassandra_std_peak_memory_index = benchmark_operation(cassandra_insert_index, large_df, 1)\n",
    "\n",
    "# --- READ ---\n",
    "read_cassandra_mean_time_index, read_cassandra_std_time_index, \\\n",
    "    read_cassandra_mean_current_memory_index, read_cassandra_std_current_memory_index, \\\n",
    "        read_cassandra_mean_peak_memory_index, read_cassandra_std_peak_memory_index = benchmark_operation(cassandra_read_index, large_df, 1)\n",
    "\n",
    "# --- UPDATE ---\n",
    "update_cassandra_mean_time_index, update_cassandra_std_time_index, \\\n",
    "    update_cassandra_mean_current_memory_index, update_cassandra_std_current_memory_index, \\\n",
    "        update_cassandra_mean_peak_memory_index, update_cassandra_std_peak_memory_index = benchmark_operation(cassandra_update_index, large_df, 1)\n",
    "\n",
    "# --- DELETE ---\n",
    "delete_cassandra_mean_time_index, delete_cassandra_std_time_index, \\\n",
    "    delete_cassandra_mean_current_memory_index, delete_cassandra_std_current_memory_index, \\\n",
    "        delete_cassandra_mean_peak_memory_index, delete_cassandra_std_peak_memory_index = benchmark_operation(cassandra_delete_index, large_df, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 replicas\n",
    "# --- CREATE ---\n",
    "create_cassandra_massive_mean_time_2_index, create_cassandra_massive_std_time_2_index, \\\n",
    "    create_cassandra_massive_mean_current_memory_2_index, create_cassandra_massive_std_current_memory_2_index, \\\n",
    "        create_cassandra_massive_mean_peak_memory_2_index, create_cassandra_massive_std_peak_memory_2_index = benchmark_operation(cassandra_insert_index, large_df, 2)\n",
    "\n",
    "# --- READ ---\n",
    "read_cassandra_massive_mean_time_2_index, read_cassandra_massive_std_time_2_index, \\\n",
    "    read_cassandra_massive_mean_current_memory_2_index, read_cassandra_massive_std_current_memory_2_index, \\\n",
    "        read_cassandra_massive_mean_peak_memory_2_index, read_cassandra_massive_std_peak_memory_2_index = benchmark_operation(cassandra_read_index, large_df, 2)\n",
    "\n",
    "# --- UPDATE ---\n",
    "update_cassandra_massive_mean_time_2_index, update_cassandra_massive_std_time_2_index, \\\n",
    "    update_cassandra_massive_mean_current_memory_2_index, update_cassandra_massive_std_current_memory_2_index, \\\n",
    "        update_cassandra_massive_mean_peak_memory_2_index, update_cassandra_massive_std_peak_memory_2_index = benchmark_operation(cassandra_update_index, large_df, 2)\n",
    "\n",
    "# --- DELETE ---\n",
    "delete_cassandra_massive_mean_time_2_index, delete_cassandra_massive_std_time_2_index, \\\n",
    "    delete_cassandra_massive_mean_current_memory_2_index, delete_cassandra_massive_std_current_memory_2_index, \\\n",
    "        delete_cassandra_massive_mean_peak_memory_2_index, delete_cassandra_massive_std_peak_memory_2_index = benchmark_operation(cassandra_delete_index, large_df, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 replicas\n",
    "# --- CREATE ---\n",
    "create_cassandra_massive_mean_time_5_index, create_cassandra_massive_std_time_5_index, \\\n",
    "    create_cassandra_massive_mean_current_memory_5_index, create_cassandra_massive_std_current_memory_5_index, \\\n",
    "        create_cassandra_massive_mean_peak_memory_5_index, create_cassandra_massive_std_peak_memory_5_index = benchmark_operation(cassandra_insert_index, large_df, 5)\n",
    "\n",
    "# --- READ ---\n",
    "read_cassandra_massive_mean_time_5_index, read_cassandra_massive_std_time_5_index, \\\n",
    "    read_cassandra_massive_mean_current_memory_5_index, read_cassandra_massive_std_current_memory_5_index, \\\n",
    "        read_cassandra_massive_mean_peak_memory_5_index, read_cassandra_massive_std_peak_memory_5_index = benchmark_operation(cassandra_read_index, large_df, 5)\n",
    "\n",
    "# --- UPDATE ---\n",
    "update_cassandra_massive_mean_time_5_index, update_cassandra_massive_std_time_5_index, \\\n",
    "    update_cassandra_massive_mean_current_memory_5_index, update_cassandra_massive_std_current_memory_5_index, \\\n",
    "        update_cassandra_massive_mean_peak_memory_5_index, update_cassandra_massive_std_peak_memory_5_index = benchmark_operation(cassandra_update_index, large_df, 5)\n",
    "\n",
    "# --- DELETE ---\n",
    "delete_cassandra_massive_mean_time_5_index, delete_cassandra_massive_std_time_5_index, \\\n",
    "    delete_cassandra_massive_mean_current_memory_5_index, delete_cassandra_massive_std_current_memory_5_index, \\\n",
    "        delete_cassandra_massive_mean_peak_memory_5_index, delete_cassandra_massive_std_peak_memory_5_index = benchmark_operation(cassandra_delete_index, large_df, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comparaison des temps avec et sans index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = ['Insert', 'Read', 'Update', 'Delete']\n",
    "\n",
    "mean_without_index = [create_sqlite_massive_mean_time, read_sqlite_massive_mean_time, \n",
    "                      update_sqlite_massive_mean_time, delete_sqlite_massive_mean_time]\n",
    "mean_with_index = [create_sqlite_massive_mean_time_index, read_sqlite_massive_mean_time_index, \n",
    "                   update_sqlite_massive_mean_time_index, delete_sqlite_massive_mean_time_index]\n",
    "\n",
    "std_without_index = [create_sqlite_massive_std_time, read_sqlite_massive_std_time, \n",
    "                     update_sqlite_massive_std_time, delete_sqlite_massive_std_time]\n",
    "std_with_index = [create_sqlite_massive_std_time_index, read_sqlite_massive_std_time_index, \n",
    "                  update_sqlite_massive_std_time_index, delete_sqlite_massive_std_time_index]\n",
    "\n",
    "x = np.arange(len(actions))\n",
    "width = 0.35  \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "\n",
    "bars1 = ax.barh(x - width/2, mean_without_index, height=width, xerr=std_without_index, label='Sans Index', capsize=5, color='tab:blue')\n",
    "bars2 = ax.barh(x + width/2, mean_with_index, height=width, xerr=std_with_index, label='Avec Index', capsize=5, color='#89CFF0')\n",
    "\n",
    "ax.set_ylabel('Actions')\n",
    "ax.set_xlabel('Temps moyen (en secondes)')\n",
    "ax.set_title('Comparaison des performances avec et sans indexation (SQLite)')\n",
    "ax.set_yticks(x)\n",
    "ax.set_yticklabels(actions)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = ['Insert', 'Read', 'Update', 'Delete']\n",
    "\n",
    "mean_without_index = [create_cas_mean_time, read_cas_mean_time, \n",
    "                      update_cas_mean_time, delete_cas_mean_time]\n",
    "mean_with_index = [create_cassandra_mean_time_index, read_cassandra_mean_time_index, \n",
    "                   update_cassandra_mean_time_index, delete_cassandra_mean_time_index]\n",
    "\n",
    "std_without_index = [create_cas_std_time, read_cas_std_time, \n",
    "                     update_cas_std_time, delete_cas_std_time]\n",
    "std_with_index = [create_cassandra_std_time_index, read_cassandra_std_time_index, \n",
    "                  update_cassandra_std_time_index, delete_cassandra_std_time_index]\n",
    "\n",
    "x = np.arange(len(actions))\n",
    "width = 0.35  \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "\n",
    "bars1 = ax.barh(x - width/2, mean_without_index, height=width, xerr=std_without_index, label='Sans Index', capsize=5,color='tab:red')\n",
    "bars2 = ax.barh(x + width/2, mean_with_index, height=width, xerr=std_with_index, label='Avec Index', capsize=5, color='#FFCCBB')\n",
    "\n",
    "ax.set_ylabel('Actions')\n",
    "ax.set_xlabel('Temps moyen (en secondes)')\n",
    "ax.set_title('Comparaison des performances avec et sans indexation (Cassandra 1 replica)')\n",
    "ax.set_yticks(x)\n",
    "ax.set_yticklabels(actions)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = ['Insert', 'Read', 'Update', 'Delete']\n",
    "\n",
    "mean_without_index = [create_cassandra_massive_mean_time_2, read_cassandra_massive_mean_time_2, \n",
    "                      update_cassandra_massive_mean_time_2, delete_cassandra_massive_mean_time_2]\n",
    "mean_with_index = [create_cassandra_massive_mean_time_2_index, read_cassandra_massive_mean_time_2_index, \n",
    "                   update_cassandra_massive_mean_time_2_index, delete_cassandra_massive_mean_time_2_index]\n",
    "\n",
    "std_without_index = [create_cassandra_massive_std_time_2, read_cassandra_massive_std_time_2, \n",
    "                     update_cassandra_massive_std_time_2, delete_cassandra_massive_std_time_2]\n",
    "std_with_index = [create_cassandra_massive_std_time_2_index, read_cassandra_massive_std_time_2_index, \n",
    "                  update_cassandra_massive_std_time_2_index, delete_cassandra_massive_std_time_2_index]\n",
    "\n",
    "x = np.arange(len(actions))\n",
    "width = 0.35  \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "\n",
    "bars1 = ax.barh(x - width/2, mean_without_index, height=width, xerr=std_without_index, label='Sans Index', capsize=5, color='tab:green')\n",
    "bars2 = ax.barh(x + width/2, mean_with_index, height=width, xerr=std_with_index, label='Avec Index', capsize=5, color='#90EE90')\n",
    "\n",
    "ax.set_ylabel('Actions')\n",
    "ax.set_xlabel('Temps moyen (en secondes)')\n",
    "ax.set_title('Comparaison des performances avec et sans indexation (Cassandra 2 replica)')\n",
    "ax.set_yticks(x)\n",
    "ax.set_yticklabels(actions)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = ['Insert', 'Read', 'Update', 'Delete']\n",
    "\n",
    "mean_without_index = [create_cassandra_massive_mean_time_5, read_cassandra_massive_mean_time_5, \n",
    "                      update_cassandra_massive_mean_time_5, delete_cassandra_massive_mean_time_5]\n",
    "mean_with_index = [create_cassandra_massive_mean_time_5_index, read_cassandra_massive_mean_time_5_index, \n",
    "                   update_cassandra_massive_mean_time_5_index, delete_cassandra_massive_mean_time_5_index]\n",
    "\n",
    "\n",
    "std_without_index = [create_cassandra_massive_std_time_5, read_cassandra_massive_std_time_5, \n",
    "                     update_cassandra_massive_std_time_5, delete_cassandra_massive_std_time_5]\n",
    "std_with_index = [create_cassandra_massive_std_time_5_index, read_cassandra_massive_std_time_5_index, \n",
    "                  update_cassandra_massive_std_time_5_index, delete_cassandra_massive_std_time_5_index]\n",
    "\n",
    "x = np.arange(len(actions))\n",
    "width = 0.35  \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "\n",
    "bars1 = ax.barh(x - width/2, mean_without_index, height=width, xerr=std_without_index, label='Sans Index', capsize=5, color='tab:orange')\n",
    "bars2 = ax.barh(x + width/2, mean_with_index, height=width, xerr=std_with_index, label='Avec Index', capsize=5, color='#FFD580')\n",
    "\n",
    "ax.set_ylabel('Actions')\n",
    "ax.set_xlabel('Temps moyen (en secondes)')\n",
    "ax.set_title('Comparaison des performances avec et sans indexation (Cassandra 5 replica)')\n",
    "ax.set_yticks(x)\n",
    "ax.set_yticklabels(actions)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comparaison de la mémoire avec et sans les index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "actions = ['Insert', 'Read', 'Update', 'Delete']\n",
    "\n",
    "# Moyennes et écarts types pour la mémoire actuelle\n",
    "mean_current_without_index = [create_sqlite_massive_mean_current_memory, read_sqlite_massive_mean_current_memory, \n",
    "                              update_sqlite_massive_mean_current_memory, delete_sqlite_massive_mean_current_memory]\n",
    "mean_current_with_index = [create_sqlite_massive_mean_current_memory_index, read_sqlite_massive_mean_current_memory_index, \n",
    "                           update_sqlite_massive_mean_current_memory_index, delete_sqlite_massive_mean_current_memory_index]\n",
    "std_current_without_index = [create_sqlite_massive_std_current_memory, read_sqlite_massive_std_current_memory, \n",
    "                             update_sqlite_massive_std_current_memory, delete_sqlite_massive_std_current_memory]\n",
    "std_current_with_index = [create_sqlite_massive_std_current_memory_index, read_sqlite_massive_std_current_memory_index, \n",
    "                          update_sqlite_massive_std_current_memory_index, delete_sqlite_massive_std_current_memory_index]\n",
    "\n",
    "# Moyennes et écarts types pour la mémoire de pointe\n",
    "mean_peak_without_index = [create_sqlite_massive_mean_peak_memory, read_sqlite_massive_mean_peak_memory, \n",
    "                           update_sqlite_massive_mean_peak_memory, delete_sqlite_massive_mean_peak_memory]\n",
    "mean_peak_with_index = [create_sqlite_massive_mean_peak_memory_index, read_sqlite_massive_mean_peak_memory_index, \n",
    "                        update_sqlite_massive_mean_peak_memory_index, delete_sqlite_massive_mean_peak_memory_index]\n",
    "std_peak_without_index = [create_sqlite_massive_std_peak_memory, read_sqlite_massive_std_peak_memory, \n",
    "                          update_sqlite_massive_std_peak_memory, delete_sqlite_massive_std_peak_memory]\n",
    "std_peak_with_index = [create_sqlite_massive_std_peak_memory_index, read_sqlite_massive_std_peak_memory_index, \n",
    "                       update_sqlite_massive_std_peak_memory_index, delete_sqlite_massive_std_peak_memory_index]\n",
    "\n",
    "x = np.arange(len(actions))\n",
    "width = 0.2  # Réduire la largeur pour accueillir plus de barres\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Barres pour la mémoire actuelle\n",
    "bars_current_without = ax.barh(x - width, mean_current_without_index, height=width, xerr=std_current_without_index, \n",
    "                               label='Mémoire actuelle sans index', capsize=5, color='tab:blue')\n",
    "bars_current_with = ax.barh(x, mean_current_with_index, height=width, xerr=std_current_with_index, \n",
    "                            label='Mémoire actuelle avec index', capsize=5, color='#89CFF0')\n",
    "\n",
    "# Barres pour la mémoire de pointe\n",
    "bars_peak_without = ax.barh(x + width, mean_peak_without_index, height=width, xerr=std_peak_without_index, \n",
    "                            label='Mémoire de pointe sans index', capsize=5, color='pink')\n",
    "bars_peak_with = ax.barh(x + 2 * width, mean_peak_with_index, height=width, xerr=std_peak_with_index, \n",
    "                         label='Mémoire de pointe avec index', capsize=5, color='gold')\n",
    "\n",
    "# Ajustements des axes et légende\n",
    "ax.set_ylabel('Actions')\n",
    "ax.set_xlabel('Mémoire (en Mo)')\n",
    "ax.set_title('Comparaison de la mémoire actuelle et de pointe avec et sans indexation (SQLite)')\n",
    "ax.set_yticks(x)\n",
    "ax.set_yticklabels(actions)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "actions = ['Insert', 'Read', 'Update', 'Delete']\n",
    "\n",
    "\n",
    "# Moyennes et écarts types pour la mémoire actuelle\n",
    "mean_current_without_index = [create_cas_mean_current_memory, read_cas_mean_current_memory, \n",
    "                              update_cas_mean_current_memory, delete_cas_mean_current_memory]\n",
    "mean_current_with_index = [create_cassandra_mean_current_memory_index, read_cassandra_mean_current_memory_index, \n",
    "                           update_cassandra_mean_current_memory_index, delete_cassandra_mean_current_memory_index]\n",
    "std_current_without_index = [create_cas_std_current_memory, read_cas_std_current_memory, \n",
    "                             update_cas_std_current_memory, delete_cas_std_current_memory]\n",
    "std_current_with_index = [create_cassandra_std_current_memory_index, read_cassandra_std_current_memory_index, \n",
    "                          update_cassandra_std_current_memory_index, delete_cassandra_std_current_memory_index]\n",
    "\n",
    "# Moyennes et écarts types pour la mémoire de pointe\n",
    "mean_peak_without_index = [create_cas_mean_peak_memory, read_cas_mean_peak_memory, \n",
    "                           update_cas_mean_peak_memory, delete_cas_mean_peak_memory]\n",
    "mean_peak_with_index = [create_cassandra_mean_peak_memory_index, read_cassandra_mean_peak_memory_index, \n",
    "                        update_cassandra_mean_peak_memory_index, delete_cassandra_mean_peak_memory_index]\n",
    "std_peak_without_index = [create_cas_std_peak_memory, read_cas_std_peak_memory, \n",
    "                          update_cas_std_peak_memory, delete_cas_std_peak_memory]\n",
    "std_peak_with_index = [create_cassandra_std_peak_memory_index, read_cassandra_std_peak_memory_index, \n",
    "                       update_cassandra_std_peak_memory_index, delete_cassandra_std_peak_memory_index]\n",
    "\n",
    "x = np.arange(len(actions))\n",
    "width = 0.2  # Réduire la largeur pour accueillir plus de barres\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Barres pour la mémoire actuelle\n",
    "bars_current_without = ax.barh(x - width, mean_current_without_index, height=width, xerr=std_current_without_index, \n",
    "                               label='Mémoire actuelle sans index', capsize=5, color='tab:red')\n",
    "bars_current_with = ax.barh(x, mean_current_with_index, height=width, xerr=std_current_with_index, \n",
    "                            label='Mémoire actuelle avec index', capsize=5, color='#FFCCBB')\n",
    "\n",
    "# Barres pour la mémoire de pointe\n",
    "bars_peak_without = ax.barh(x + width, mean_peak_without_index, height=width, xerr=std_peak_without_index, \n",
    "                            label='Mémoire de pointe sans index', capsize=5, color='green')\n",
    "bars_peak_with = ax.barh(x + 2 * width, mean_peak_with_index, height=width, xerr=std_peak_with_index, \n",
    "                         label='Mémoire de pointe avec index', capsize=5, color='gold')\n",
    "\n",
    "# Ajustements des axes et légende\n",
    "ax.set_ylabel('Actions')\n",
    "ax.set_xlabel('Mémoire (en Mo)')\n",
    "ax.set_title('Comparaison de la mémoire actuelle et de pointe avec et sans indexation (Cassandra 1 replica)')\n",
    "ax.set_yticks(x)\n",
    "ax.set_yticklabels(actions)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_cassandra_massive_mean_time_2\n",
    "update_cassandra_massive_mean_time_5_index\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "actions = ['Insert', 'Read', 'Update', 'Delete']\n",
    "\n",
    "# Moyennes et écarts types pour la mémoire actuelle\n",
    "mean_current_without_index = [create_cassandra_massive_mean_current_memory_2, read_cassandra_massive_mean_current_memory_2, \n",
    "                              update_cassandra_massive_mean_current_memory_2, delete_cassandra_massive_mean_current_memory_2]\n",
    "mean_current_with_index = [create_cassandra_massive_mean_current_memory_2_index, read_cassandra_massive_mean_current_memory_2_index, \n",
    "                           update_cassandra_massive_mean_current_memory_2_index, delete_cassandra_massive_mean_current_memory_2_index]\n",
    "std_current_without_index = [create_cassandra_massive_std_current_memory_2, read_cassandra_massive_std_current_memory_2, \n",
    "                             update_cassandra_massive_std_current_memory_2, delete_cassandra_massive_std_current_memory_2]\n",
    "std_current_with_index = [create_cassandra_massive_std_current_memory_2_index, read_cassandra_massive_std_current_memory_2_index, \n",
    "                          update_cassandra_massive_std_current_memory_2_index, delete_cassandra_massive_std_current_memory_2_index]\n",
    "\n",
    "# Moyennes et écarts types pour la mémoire de pointe\n",
    "mean_peak_without_index = [create_cassandra_massive_mean_peak_memory_2, read_cassandra_massive_mean_peak_memory_2, \n",
    "                           update_cassandra_massive_mean_peak_memory_2, delete_cassandra_massive_mean_peak_memory_2]\n",
    "mean_peak_with_index =  [create_cassandra_massive_mean_peak_memory_2_index, read_cassandra_massive_mean_peak_memory_2_index, \n",
    "                           update_cassandra_massive_mean_peak_memory_2_index, delete_cassandra_massive_mean_peak_memory_2_index]\n",
    "std_peak_without_index = [create_cassandra_massive_std_peak_memory_2, read_cassandra_massive_std_peak_memory_2, \n",
    "                          update_cassandra_massive_std_peak_memory_2, delete_cassandra_massive_std_peak_memory_2]\n",
    "std_peak_with_index = [create_cassandra_massive_std_peak_memory_2_index, read_cassandra_massive_std_peak_memory_2_index, \n",
    "                          update_cassandra_massive_std_peak_memory_2_index, delete_cassandra_massive_std_peak_memory_2_index]\n",
    "\n",
    "x = np.arange(len(actions))\n",
    "width = 0.2  # Réduire la largeur pour accueillir plus de barres\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Barres pour la mémoire actuelle\n",
    "bars_current_without = ax.barh(x - width, mean_current_without_index, height=width, xerr=std_current_without_index, \n",
    "                               label='Mémoire actuelle sans index', capsize=5, color='tab:green')\n",
    "bars_current_with = ax.barh(x, mean_current_with_index, height=width, xerr=std_current_with_index, \n",
    "                            label='Mémoire actuelle avec index', capsize=5, color='#90EE90')\n",
    "\n",
    "# Barres pour la mémoire de pointe\n",
    "bars_peak_without = ax.barh(x + width, mean_peak_without_index, height=width, xerr=std_peak_without_index, \n",
    "                            label='Mémoire de pointe sans index', capsize=5, color='pink')\n",
    "bars_peak_with = ax.barh(x + 2 * width, mean_peak_with_index, height=width, xerr=std_peak_with_index, \n",
    "                         label='Mémoire de pointe avec index', capsize=5, color='gold')\n",
    "\n",
    "# Ajustements des axes et légende\n",
    "ax.set_ylabel('Actions')\n",
    "ax.set_xlabel('Mémoire (en Mo)')\n",
    "ax.set_title('Comparaison de la mémoire actuelle et de pointe avec et sans indexation (Cassandra 2 replica)')\n",
    "ax.set_yticks(x)\n",
    "ax.set_yticklabels(actions)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_cassandra_massive_mean_time_2\n",
    "update_cassandra_massive_mean_time_5_index\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "actions = ['Insert', 'Read', 'Update', 'Delete']\n",
    "\n",
    "# Moyennes et écarts types pour la mémoire actuelle\n",
    "mean_current_without_index = [create_cassandra_massive_mean_current_memory_5, read_cassandra_massive_mean_current_memory_5, \n",
    "                              update_cassandra_massive_mean_current_memory_5, delete_cassandra_massive_mean_current_memory_5]\n",
    "mean_current_with_index = [create_cassandra_massive_mean_current_memory_5_index, read_cassandra_massive_mean_current_memory_5_index, \n",
    "                           update_cassandra_massive_mean_current_memory_5_index, delete_cassandra_massive_mean_current_memory_5_index]\n",
    "std_current_without_index = [create_cassandra_massive_std_current_memory_5, read_cassandra_massive_std_current_memory_5, \n",
    "                             update_cassandra_massive_std_current_memory_5, delete_cassandra_massive_std_current_memory_5]\n",
    "std_current_with_index = [create_cassandra_massive_std_current_memory_5_index, read_cassandra_massive_std_current_memory_5_index, \n",
    "                          update_cassandra_massive_std_current_memory_5_index, delete_cassandra_massive_std_current_memory_5_index]\n",
    "\n",
    "# Moyennes et écarts types pour la mémoire de pointe\n",
    "mean_peak_without_index = [create_cassandra_massive_mean_peak_memory_5, read_cassandra_massive_mean_peak_memory_5, \n",
    "                           update_cassandra_massive_mean_peak_memory_5, delete_cassandra_massive_mean_peak_memory_5]\n",
    "mean_peak_with_index =  [create_cassandra_massive_mean_peak_memory_5_index, read_cassandra_massive_mean_peak_memory_5_index, \n",
    "                           update_cassandra_massive_mean_peak_memory_5_index, delete_cassandra_massive_mean_peak_memory_5_index]\n",
    "std_peak_without_index = [create_cassandra_massive_std_peak_memory_5, read_cassandra_massive_std_peak_memory_5, \n",
    "                          update_cassandra_massive_std_peak_memory_5, delete_cassandra_massive_std_peak_memory_5]\n",
    "std_peak_with_index = [create_cassandra_massive_std_peak_memory_5_index, read_cassandra_massive_std_peak_memory_5_index, \n",
    "                          update_cassandra_massive_std_peak_memory_5_index, delete_cassandra_massive_std_peak_memory_5_index]\n",
    "\n",
    "x = np.arange(len(actions))\n",
    "width = 0.2  # Réduire la largeur pour accueillir plus de barres\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Barres pour la mémoire actuelle\n",
    "bars_current_without = ax.barh(x - width, mean_current_without_index, height=width, xerr=std_current_without_index, \n",
    "                               label='Mémoire actuelle sans index', capsize=5, color='tab:orange')\n",
    "bars_current_with = ax.barh(x, mean_current_with_index, height=width, xerr=std_current_with_index, \n",
    "                            label='Mémoire actuelle avec index', capsize=5, color='#FFD580')\n",
    "\n",
    "# Barres pour la mémoire de pointe\n",
    "bars_peak_without = ax.barh(x + width, mean_peak_without_index, height=width, xerr=std_peak_without_index, \n",
    "                            label='Mémoire de pointe sans index', capsize=5, color='pink')\n",
    "bars_peak_with = ax.barh(x + 2 * width, mean_peak_with_index, height=width, xerr=std_peak_with_index, \n",
    "                         label='Mémoire de pointe avec index', capsize=5, color='red')\n",
    "\n",
    "# Ajustements des axes et légende\n",
    "ax.set_ylabel('Actions')\n",
    "ax.set_xlabel('Mémoire (en Mo)')\n",
    "ax.set_title('Comparaison de la mémoire actuelle et de pointe avec et sans indexation (Cassandra 5 replica)')\n",
    "ax.set_yticks(x)\n",
    "ax.set_yticklabels(actions)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO : il a fallu passer par clef primaire blablabla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons tenté d'inclure `release_year` dans la clé primaire (partition et clustering) afin d'éviter l'utilisation de `ALLOW FILTERING`. Cependant, cela s'est révélé compliqué à mettre en œuvre dans notre cas. Une alternative aurait été de se limiter à des requêtes utilisant uniquement la clé primaire simple `show_id`, mais cette approche a peu d'utilité pratique dans la réalité.\n",
    "\n",
    "Ainsi, nous avons conservé la version où `release_year > 2000`, même si cette condition nécessite un parcours partiel des partitions, ce qui n'est pas pris en charge nativement sans `ALLOW FILTERING`.\n",
    "\n",
    "Une autre option aurait été d'utiliser une vue matérialisée pour optimiser les requêtes sur `release_year`, mais nous avons préféré une approche où nous récupérons d'abord l'identifiant (`show_id`) avant toute opération de mise à jour ou de suppression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "Xr4EKrp9SaXl"
   },
   "outputs": [],
   "source": [
    "# TODO Charlotte tu veux dire quoi ?\n",
    "# Tester avec des requêtes plus complexes\n",
    "# Utiliser un dataset plus grand (ça c'est au dessus)\n",
    "#  Ajouter des colonnes à indexer (c'est fait aussi au dessus)\n",
    "# Tous ces TODO n'ont rien changé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YUKaVNidSaXl"
   },
   "source": [
    "# Fin du notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K__CbN_ISaXl"
   },
   "outputs": [],
   "source": [
    "# fermer proprement\n",
    "session.shutdown() # Fermer la connexion Cassandra\n",
    "cluster.shutdown() # Fermer le cluster Cassandra\n",
    "conn.close() # Fermer la connexion SQLite\n",
    "cursor.close() # Fermer le curseur SQLite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO en haut : attention notebook très long à cause du benchmark : beaucoup de cretae, insert à chaque fois sur 30000 données -> lourd "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "zNbLEjRUSaXd",
    "DSa5TnVxSaXf",
    "gyQhyPcqXcdW",
    "joVbr__Z1aEJ"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
